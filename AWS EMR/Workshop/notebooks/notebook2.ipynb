{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab : Batch Updates to a S3 Datalake using Apache Hudi\n",
    "\n",
    "# Copy on Write \n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "- 1. [Overview](#Overview)\n",
    "- 2. [Copy On Write](#Copy-On-Write)\n",
    "   - 2.1 [Bulk Insert the Initial Dataset](#Bulk-Insert-the-Initial-Dataset)\n",
    "   - 2.2 [Batch Upsert some records](#Batch-Upsert-some-records)\n",
    "   - 2.3 [Incremental-View](#Incremental-View )\n",
    "   - 2.3 [Deleting Records](#Deleting-Records.)  \n",
    "- 3. [Rollback](#Rollback)\n",
    "- 4. [Advanced - Understanding Hudi Commits](#Advanced:Understanding-Hudi-Commits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates using PySpark on [Apache Hudi](https://aws.amazon.com/emr/features/hudi/) on Amazon EMR to insert/upsert/delete records to an S3 data lake.\n",
    "\n",
    "Here is a good reference link to read later:\n",
    "\n",
    "* [How Hudi Works](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-how-it-works.html)\n",
    "\n",
    "This notebook covers the following concepts when writing Copy On Write tables to an S3 Datalake:\n",
    "\n",
    "- Write Hudi Spark jobs in PySpark.\n",
    "- Bulk Insert the Initial Dataset.\n",
    "- Write a Non-Partitioned table.\n",
    "- Sync the Hudi tables to the Hive/Glue Catalog.\n",
    "- Upsert some records to a Hudi table.\n",
    "- Delete records from a Hudi table.\n",
    "- Rollback.\n",
    "- Understanding Hudi commits.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-requisites\n",
    "\n",
    "### This demo is based on Hudi version 0.8.0 and runs fine on Jupyter Notebooks connected to a 1 node (r5.4xlarge) EMR cluster with configuration listed below \n",
    "\n",
    " - EMR versions 6.5.0 \n",
    " \n",
    " - Software configuration\n",
    "\n",
    "       - Hadoop 3.2.1\n",
    "       - Hive 3.1.2\n",
    "       - Livy 0.7.1\n",
    "       - JupyterHub 1.4.1\n",
    "       - Spark 3.1.2\n",
    "       \n",
    "       \n",
    " - AWS Glue Data Catalog settings - Select the below listed check boxes\n",
    "       - Use for Hive table metadata  \n",
    "       - Use for Spark table metadata\n",
    "\n",
    "\n",
    "\n",
    "### Connect to the Master Node of EMR cluster Using SSH :\n",
    "    - ssh -i ~/xxxx.pem hadoop@<ec2-xx-xxx-xx-xx.us-west-2.compute.amazonaws.com>\n",
    "\n",
    "    - Ensure  the below listed files are copied into HDFS.\n",
    "\n",
    "    - hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar hdfs:///user/hadoop/\n",
    "\n",
    "    - hdfs dfs -copyFromLocal /usr/lib/spark/external/lib/spark-avro.jar hdfs:///user/hadoop/\n",
    "    \n",
    "    - hdfs dfs -copyFromLocal /usr/share/aws/aws-java-sdk/aws-java-sdk-bundle-1.12.31.jar hdfs:///user/hadoop/\n",
    "\n",
    "- https://github.com/apache/hudi/issues/5053\n",
    "\n",
    "Let's start by initializing the Spark Session to connect this notebook to our Spark EMR cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>6</td><td>application_1648194189527_0007</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-77-205.ec2.internal:20888/proxy/application_1648194189527_0007/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-77-205.ec2.internal:8042/node/containerlogs/container_1648194189527_0007_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///user/hadoop/aws-java-sdk-bundle-1.12.31.jar, hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar', 'spark.sql.hive.convertMetastoreParquet': 'false', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.dynamicAllocation.executorIdleTimeout': 3600, 'spark.executor.memory': '5G', 'spark.executor.cores': 3, 'spark.dynamicAllocation.initialExecutors': 5}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>4</td><td>application_1648194189527_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-77-205.ec2.internal:20888/proxy/application_1648194189527_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-77-205.ec2.internal:8042/node/containerlogs/container_1648194189527_0005_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>6</td><td>application_1648194189527_0007</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-77-205.ec2.internal:20888/proxy/application_1648194189527_0007/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-77-205.ec2.internal:8042/node/containerlogs/container_1648194189527_0007_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars\":\"hdfs:///user/hadoop/aws-java-sdk-bundle-1.12.31.jar, hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar\",\n",
    "             \"spark.sql.hive.convertMetastoreParquet\":\"false\",     \n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "             \"spark.dynamicAllocation.executorIdleTimeout\": 3600,\n",
    "             \"spark.executor.memory\": \"5G\",\n",
    "             \"spark.executor.cores\": 3,\n",
    "             \"spark.dynamicAllocation.initialExecutors\":5\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you update the bucket name to your unique bucket before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## CHANGE ME ##\n",
    "config = {\n",
    "    \"table_name\": \"hudi_trips_table\",\n",
    "    \"target\": \"s3://<Your S3 Bucket Here>/hudi/hudi_trips_table\",\n",
    "    \"primary_key\": \"trip_id\",\n",
    "    \"sort_key\": \"tstamp\",\n",
    "    \"commits_to_retain\": \"4\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constants for Python to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General Constants\n",
    "HUDI_FORMAT = \"org.apache.hudi\"\n",
    "TABLE_NAME = \"hoodie.table.name\"\n",
    "RECORDKEY_FIELD_OPT_KEY = \"hoodie.datasource.write.recordkey.field\"\n",
    "PRECOMBINE_FIELD_OPT_KEY = \"hoodie.datasource.write.precombine.field\"\n",
    "OPERATION_OPT_KEY = \"hoodie.datasource.write.operation\"\n",
    "BULK_INSERT_OPERATION_OPT_VAL = \"bulk_insert\"\n",
    "UPSERT_OPERATION_OPT_VAL = \"upsert\"\n",
    "DELETE_OPERATION_OPT_VAL = \"delete\"\n",
    "BULK_INSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\"\n",
    "UPSERT_PARALLELISM = \"hoodie.upsert.shuffle.parallelism\"\n",
    "S3_CONSISTENCY_CHECK = \"hoodie.consistency.check.enabled\"\n",
    "HUDI_CLEANER_POLICY = \"hoodie.cleaner.policy\"\n",
    "KEEP_LATEST_COMMITS = \"KEEP_LATEST_COMMITS\"\n",
    "KEEP_LATEST_FILE_VERSIONS = \"KEEP_LATEST_FILE_VERSIONS\"\n",
    "HUDI_COMMITS_RETAINED = \"hoodie.cleaner.commits.retained\"\n",
    "HUDI_FILES_RETAINED = \"hoodie.cleaner.fileversions.retained\"\n",
    "PAYLOAD_CLASS_OPT_KEY = \"hoodie.datasource.write.payload.class.key()\"\n",
    "EMPTY_PAYLOAD_CLASS_OPT_VAL = \"org.apache.hudi.EmptyHoodieRecordPayload\"\n",
    "\n",
    "# Hive Constants\n",
    "HIVE_SYNC_ENABLED_OPT_KEY=\"hoodie.datasource.hive_sync.enable\"\n",
    "HIVE_PARTITION_FIELDS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_fields\"\n",
    "HIVE_ASSUME_DATE_PARTITION_OPT_KEY=\"hoodie.datasource.hive_sync.assume_date_partitioning\"\n",
    "HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_extractor_class\"\n",
    "HIVE_TABLE_OPT_KEY=\"hoodie.datasource.hive_sync.table\"\n",
    "\n",
    "# Partition Constants\n",
    "NONPARTITION_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.NonPartitionedExtractor\"\n",
    "MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.MultiPartKeysValueExtractor\"\n",
    "KEYGENERATOR_CLASS_OPT_KEY=\"hoodie.datasource.write.keygenerator.class\"\n",
    "NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.keygen.NonpartitionedKeyGenerator\"\n",
    "COMPLEX_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.ComplexKeyGenerator\"\n",
    "PARTITIONPATH_FIELD_OPT_KEY=\"hoodie.datasource.write.partitionpath.field\"\n",
    "\n",
    "#Incremental Constants\n",
    "VIEW_TYPE_OPT_KEY=\"hoodie.datasource.query.type\"\n",
    "BEGIN_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.begin.instanttime\"\n",
    "VIEW_TYPE_INCREMENTAL_OPT_VAL=\"incremental\"\n",
    "END_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.end.instanttime\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to create JSON data and Spark dataframe from this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generates Data\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_json_data(start, count, dest):\n",
    "    time_stamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    data = [{\"trip_id\": i, \"tstamp\": time_stamp, \"route_id\": chr(65 + (i % 10)), \"destination\": dest[i%10]} for i in range(start, start + count)]\n",
    "    return data\n",
    "\n",
    "# Creates the Dataframe\n",
    "def create_json_df(spark, data):\n",
    "    sc = spark.sparkContext\n",
    "    return spark.read.json(sc.parallelize(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk Insert the Initial Dataset\n",
    "\n",
    "Let's generate 2M records to load into our Data Lake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000\n",
      "+-------------+--------+-------+-------------------+\n",
      "|  destination|route_id|trip_id|             tstamp|\n",
      "+-------------+--------+-------+-------------------+\n",
      "|      Seattle|       A|      0|2022-03-25 09:40:07|\n",
      "|     New York|       B|      1|2022-03-25 09:40:07|\n",
      "|   New Jersey|       C|      2|2022-03-25 09:40:07|\n",
      "|  Los Angeles|       D|      3|2022-03-25 09:40:07|\n",
      "|    Las Vegas|       E|      4|2022-03-25 09:40:07|\n",
      "|       Tucson|       F|      5|2022-03-25 09:40:07|\n",
      "|Washington DC|       G|      6|2022-03-25 09:40:07|\n",
      "| Philadelphia|       H|      7|2022-03-25 09:40:07|\n",
      "|        Miami|       I|      8|2022-03-25 09:40:07|\n",
      "|San Francisco|       J|      9|2022-03-25 09:40:07|\n",
      "|      Seattle|       A|     10|2022-03-25 09:40:07|\n",
      "|     New York|       B|     11|2022-03-25 09:40:07|\n",
      "|   New Jersey|       C|     12|2022-03-25 09:40:07|\n",
      "|  Los Angeles|       D|     13|2022-03-25 09:40:07|\n",
      "|    Las Vegas|       E|     14|2022-03-25 09:40:07|\n",
      "|       Tucson|       F|     15|2022-03-25 09:40:07|\n",
      "|Washington DC|       G|     16|2022-03-25 09:40:07|\n",
      "| Philadelphia|       H|     17|2022-03-25 09:40:07|\n",
      "|        Miami|       I|     18|2022-03-25 09:40:07|\n",
      "|San Francisco|       J|     19|2022-03-25 09:40:07|\n",
      "+-------------+--------+-------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "dest = [\"Seattle\", \"New York\", \"New Jersey\", \"Los Angeles\", \"Las Vegas\", \"Tucson\",\"Washington DC\",\"Philadelphia\",\"Miami\",\"San Francisco\"]\n",
    "df1 = create_json_df(spark, get_json_data(0, 2000000, dest))\n",
    "print(df1.count())\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy On Write\n",
    "\n",
    "The default table type is Copy-On-Write which is best suited for read-heavy workloads with modest writes. Copy-On-Write creates commit files with original data + the new changes during writing itself. While this increases latency on writes, this set up makes it more manageable for faster read.\n",
    "\n",
    "\n",
    "We will be using the Copy on write storage option(STORAGE_TYPE_OPT_KEY, \"COPY_ON_WRITE\") which is the default option\n",
    "and need not be explicitly set.\n",
    "\n",
    "Write the data to S3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 3)\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the number of files in S3. Expected number of files is 3 files as BULK_INSERT_PARALLELISM is set to 3. \n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://<Your S3 Bucket Here>/tmp/hudi/hudi_trips_table/ --summarize --human-readable\n",
    "                           PRE .hoodie/\n",
    "2021-11-15 19:47:08          0 .hoodie_$folder$\n",
    "2021-11-15 19:47:19         93 .hoodie_partition_metadata\n",
    "2021-11-15 19:47:29    4874333 2ac15cd9-60ce-4fbd-8ae7-91abae3a3a12-0_2-15-105_20211115194706.parquet\n",
    "2021-11-15 19:47:29    4936875 3de41050-e3d7-41d2-8ce5-2892ac24e2e1-0_1-15-104_20211115194706.parquet\n",
    "2021-11-15 19:47:26    4672448 bfa09b26-b3af-4f1a-90d1-c6f71bf70a07-0_0-15-103_20211115194706.parquet\n",
    "\n",
    "Total Objects: 5\n",
    "   Total Size: 13.8 MiB\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "Let's inspect the table created and query the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                  |comment|\n",
      "+----------------------------+-----------------------------------------------------------+-------+\n",
      "|_hoodie_commit_time         |string                                                     |null   |\n",
      "|_hoodie_commit_seqno        |string                                                     |null   |\n",
      "|_hoodie_record_key          |string                                                     |null   |\n",
      "|_hoodie_partition_path      |string                                                     |null   |\n",
      "|_hoodie_file_name           |string                                                     |null   |\n",
      "|destination                 |string                                                     |null   |\n",
      "|route_id                    |string                                                     |null   |\n",
      "|trip_id                     |bigint                                                     |null   |\n",
      "|tstamp                      |string                                                     |null   |\n",
      "|                            |                                                           |       |\n",
      "|# Detailed Table Information|                                                           |       |\n",
      "|Database                    |default                                                    |       |\n",
      "|Table                       |hudi_trips_table_v2                                        |       |\n",
      "|Owner                       |hive                                                       |       |\n",
      "|Created Time                |Fri Mar 25 08:07:03 UTC 2022                               |       |\n",
      "|Last Access                 |UNKNOWN                                                    |       |\n",
      "|Created By                  |Spark 2.2 or prior                                         |       |\n",
      "|Type                        |EXTERNAL                                                   |       |\n",
      "|Provider                    |hudi                                                       |       |\n",
      "|Table Properties            |[bucketing_version=2, last_commit_time_sync=20220325094025]|       |\n",
      "+----------------------------+-----------------------------------------------------------+-------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe formatted \"+config['table_name']).show(20,False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note the extra columns that are added by Hudi to keep track of commits and filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+------------------+----------------------+--------------------------------------------------------------------+-------------+--------+-------+-------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno   |_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name                                                   |destination  |route_id|trip_id|tstamp             |\n",
      "+-------------------+-----------------------+------------------+----------------------+--------------------------------------------------------------------+-------------+--------+-------+-------------------+\n",
      "|20220325094025     |20220325094025_2_655243|407615            |                      |008d7dbd-649d-4355-956e-9b6b3f8b48fa-0_2-9-64_20220325094025.parquet|Tucson       |F       |407615 |2022-03-25 09:40:07|\n",
      "|20220325094025     |20220325094025_2_655244|407616            |                      |008d7dbd-649d-4355-956e-9b6b3f8b48fa-0_2-9-64_20220325094025.parquet|Washington DC|G       |407616 |2022-03-25 09:40:07|\n",
      "|20220325094025     |20220325094025_2_655245|407617            |                      |008d7dbd-649d-4355-956e-9b6b3f8b48fa-0_2-9-64_20220325094025.parquet|Philadelphia |H       |407617 |2022-03-25 09:40:07|\n",
      "|20220325094025     |20220325094025_2_655246|407618            |                      |008d7dbd-649d-4355-956e-9b6b3f8b48fa-0_2-9-64_20220325094025.parquet|Miami        |I       |407618 |2022-03-25 09:40:07|\n",
      "|20220325094025     |20220325094025_2_655247|407619            |                      |008d7dbd-649d-4355-956e-9b6b3f8b48fa-0_2-9-64_20220325094025.parquet|San Francisco|J       |407619 |2022-03-25 09:40:07|\n",
      "|20220325094025     |20220325094025_2_655248|40762             |                      |008d7dbd-649d-4355-956e-9b6b3f8b48fa-0_2-9-64_20220325094025.parquet|New Jersey   |C       |40762  |2022-03-25 09:40:07|\n",
      "|20220325094025     |20220325094025_2_655249|407620            |                      |008d7dbd-649d-4355-956e-9b6b3f8b48fa-0_2-9-64_20220325094025.parquet|Seattle      |A       |407620 |2022-03-25 09:40:07|\n",
      "|20220325094025     |20220325094025_2_655250|407621            |                      |008d7dbd-649d-4355-956e-9b6b3f8b48fa-0_2-9-64_20220325094025.parquet|New York     |B       |407621 |2022-03-25 09:40:07|\n",
      "|20220325094025     |20220325094025_2_655251|407622            |                      |008d7dbd-649d-4355-956e-9b6b3f8b48fa-0_2-9-64_20220325094025.parquet|New Jersey   |C       |407622 |2022-03-25 09:40:07|\n",
      "|20220325094025     |20220325094025_2_655252|407623            |                      |008d7dbd-649d-4355-956e-9b6b3f8b48fa-0_2-9-64_20220325094025.parquet|Los Angeles  |D       |407623 |2022-03-25 09:40:07|\n",
      "|20220325094025     |20220325094025_2_655253|407624            |                      |008d7dbd-649d-4355-956e-9b6b3f8b48fa-0_2-9-64_20220325094025.parquet|Las Vegas    |E       |407624 |2022-03-25 09:40:07|\n",
      "|20220325094025     |20220325094025_2_655254|407625            |                      |008d7dbd-649d-4355-956e-9b6b3f8b48fa-0_2-9-64_20220325094025.parquet|Tucson       |F       |407625 |2022-03-25 09:40:07|\n",
      "|20220325094025     |20220325094025_2_655255|407626            |                      |008d7dbd-649d-4355-956e-9b6b3f8b48fa-0_2-9-64_20220325094025.parquet|Washington DC|G       |407626 |2022-03-25 09:40:07|\n",
      "|20220325094025     |20220325094025_2_655256|407627            |                      |008d7dbd-649d-4355-956e-9b6b3f8b48fa-0_2-9-64_20220325094025.parquet|Philadelphia |H       |407627 |2022-03-25 09:40:07|\n",
      "|20220325094025     |20220325094025_2_655257|407628            |                      |008d7dbd-649d-4355-956e-9b6b3f8b48fa-0_2-9-64_20220325094025.parquet|Miami        |I       |407628 |2022-03-25 09:40:07|\n",
      "|20220325094025     |20220325094025_2_655258|407629            |                      |008d7dbd-649d-4355-956e-9b6b3f8b48fa-0_2-9-64_20220325094025.parquet|San Francisco|J       |407629 |2022-03-25 09:40:07|\n",
      "|20220325094025     |20220325094025_2_655259|40763             |                      |008d7dbd-649d-4355-956e-9b6b3f8b48fa-0_2-9-64_20220325094025.parquet|Los Angeles  |D       |40763  |2022-03-25 09:40:07|\n",
      "|20220325094025     |20220325094025_2_655260|407630            |                      |008d7dbd-649d-4355-956e-9b6b3f8b48fa-0_2-9-64_20220325094025.parquet|Seattle      |A       |407630 |2022-03-25 09:40:07|\n",
      "|20220325094025     |20220325094025_2_655261|407631            |                      |008d7dbd-649d-4355-956e-9b6b3f8b48fa-0_2-9-64_20220325094025.parquet|New York     |B       |407631 |2022-03-25 09:40:07|\n",
      "|20220325094025     |20220325094025_2_655262|407632            |                      |008d7dbd-649d-4355-956e-9b6b3f8b48fa-0_2-9-64_20220325094025.parquet|New Jersey   |C       |407632 |2022-03-25 09:40:07|\n",
      "+-------------------+-----------------------+------------------+----------------------+--------------------------------------------------------------------+-------------+--------+-------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df2=spark.read.format(HUDI_FORMAT).load(config[\"target\"]+\"/*\")\n",
    "df2.count()\n",
    "df2.show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query the Hive table as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|2000000 |\n",
      "+--------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from \"+config['table_name']).show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Batch Upsert some records\n",
    "\n",
    "Let's modify a few records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+-------------------+\n",
      "|trip_id|route_id|destination  |tstamp             |\n",
      "+-------+--------+-------------+-------------------+\n",
      "|1000000|A       |Seattle      |2022-03-25 09:40:07|\n",
      "|1000001|B       |New York     |2022-03-25 09:40:07|\n",
      "|1000002|C       |New Jersey   |2022-03-25 09:40:07|\n",
      "|1000003|D       |Los Angeles  |2022-03-25 09:40:07|\n",
      "|1000004|E       |Las Vegas    |2022-03-25 09:40:07|\n",
      "|1000005|F       |Tucson       |2022-03-25 09:40:07|\n",
      "|1000006|G       |Washington DC|2022-03-25 09:40:07|\n",
      "|1000007|H       |Philadelphia |2022-03-25 09:40:07|\n",
      "|1000008|I       |Miami        |2022-03-25 09:40:07|\n",
      "|1000009|J       |San Francisco|2022-03-25 09:40:07|\n",
      "+-------+--------+-------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name'] +\" where trip_id between 1000000 and 1000009\").show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10"
     ]
    }
   ],
   "source": [
    "upsert_dest = [\"Boston\", \"Boston\", \"Boston\", \"Boston\", \"Boston\",\"Boston\",\"Boston\",\"Boston\",\"Boston\",\"Boston\"]\n",
    "df3 = create_json_df(spark, get_json_data(1000000, 10, upsert_dest))\n",
    "df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+-------------------+\n",
      "|trip_id|route_id|destination|             tstamp|\n",
      "+-------+--------+-----------+-------------------+\n",
      "|1000000|       A|     Boston|2022-03-25 09:44:40|\n",
      "|1000001|       B|     Boston|2022-03-25 09:44:40|\n",
      "|1000002|       C|     Boston|2022-03-25 09:44:40|\n",
      "|1000003|       D|     Boston|2022-03-25 09:44:40|\n",
      "|1000004|       E|     Boston|2022-03-25 09:44:40|\n",
      "|1000005|       F|     Boston|2022-03-25 09:44:40|\n",
      "|1000006|       G|     Boston|2022-03-25 09:44:40|\n",
      "|1000007|       H|     Boston|2022-03-25 09:44:40|\n",
      "|1000008|       I|     Boston|2022-03-25 09:44:40|\n",
      "|1000009|       J|     Boston|2022-03-25 09:44:40|\n",
      "+-------+--------+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "df3.select(\"trip_id\",\"route_id\",\"destination\",\"tstamp\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have changed the destination and timestamp for trip IDs 1000000 to 1000010. Now, let's upsert the changes to S3. Note that the operation now is \"Upsert\" as opposed to BulkInsert for the initial load:\n",
    "\n",
    "```\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df3.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-------------+\n",
      "|trip_id|route_id|tstamp             |destination  |\n",
      "+-------+--------+-------------------+-------------+\n",
      "|1000000|A       |2022-03-25 09:44:40|Boston       |\n",
      "|1000001|B       |2022-03-25 09:44:40|Boston       |\n",
      "|1000002|C       |2022-03-25 09:44:40|Boston       |\n",
      "|1000003|D       |2022-03-25 09:44:40|Boston       |\n",
      "|1000004|E       |2022-03-25 09:44:40|Boston       |\n",
      "|1000005|F       |2022-03-25 09:44:40|Boston       |\n",
      "|1000006|G       |2022-03-25 09:44:40|Boston       |\n",
      "|1000007|H       |2022-03-25 09:44:40|Boston       |\n",
      "|1000008|I       |2022-03-25 09:44:40|Boston       |\n",
      "|1000009|J       |2022-03-25 09:44:40|Boston       |\n",
      "|1000010|A       |2022-03-25 09:40:07|Seattle      |\n",
      "|1000011|B       |2022-03-25 09:40:07|New York     |\n",
      "|1000012|C       |2022-03-25 09:40:07|New Jersey   |\n",
      "|1000013|D       |2022-03-25 09:40:07|Los Angeles  |\n",
      "|999996 |G       |2022-03-25 09:40:07|Washington DC|\n",
      "|999997 |H       |2022-03-25 09:40:07|Philadelphia |\n",
      "|999998 |I       |2022-03-25 09:40:07|Miami        |\n",
      "|999999 |J       |2022-03-25 09:40:07|San Francisco|\n",
      "+-------+--------+-------------------+-------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id between 999996 and 1000013\").show(50,False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the commit. You will notice 3 base files from Bulk Insert + 1 different version of one of the base file from upsert. For instance, in below example we now have 2 versions of bfa09b26-b3af-4f1a-90d1-c6f71bf70a07\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<Your S3 Bucket Here>/tmp/hudi/hudi_trips_table/ --summarize --human-readable\n",
    "                           PRE .hoodie/\n",
    "2021-11-15 19:47:08          0 .hoodie_$folder$\n",
    "2021-11-15 19:47:19         93 .hoodie_partition_metadata\n",
    "2021-11-15 19:47:29    4874333 2ac15cd9-60ce-4fbd-8ae7-91abae3a3a12-0_2-15-105_20211115194706.parquet\n",
    "2021-11-15 19:47:29    4936875 3de41050-e3d7-41d2-8ce5-2892ac24e2e1-0_1-15-104_20211115194706.parquet\n",
    "2021-11-15 19:47:26    4672448 bfa09b26-b3af-4f1a-90d1-c6f71bf70a07-0_0-15-103_20211115194706.parquet\n",
    "2021-11-15 19:50:29    4672282 bfa09b26-b3af-4f1a-90d1-c6f71bf70a07-0_0-59-416_20211115195018.parquet\n",
    "\n",
    "Total Objects: 6\n",
    "   Total Size: 18.6 MiB\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we upserted some records, let us try to insert 10 new records into the table. We will use same upsert option. As primary keys 2000000 to 2000009 do not exist in the table, the records will be inserted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+-------------------+\n",
      "|destination|route_id|trip_id|             tstamp|\n",
      "+-----------+--------+-------+-------------------+\n",
      "|   Syracuse|       A|2000000|2022-03-25 09:45:31|\n",
      "|   Syracuse|       B|2000001|2022-03-25 09:45:31|\n",
      "|   Syracuse|       C|2000002|2022-03-25 09:45:31|\n",
      "|   Syracuse|       D|2000003|2022-03-25 09:45:31|\n",
      "|   Syracuse|       E|2000004|2022-03-25 09:45:31|\n",
      "|   Syracuse|       F|2000005|2022-03-25 09:45:31|\n",
      "|   Syracuse|       G|2000006|2022-03-25 09:45:31|\n",
      "|   Syracuse|       H|2000007|2022-03-25 09:45:31|\n",
      "|   Syracuse|       I|2000008|2022-03-25 09:45:31|\n",
      "|   Syracuse|       J|2000009|2022-03-25 09:45:31|\n",
      "+-----------+--------+-------+-------------------+"
     ]
    }
   ],
   "source": [
    "insert_dest = [\"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\"]\n",
    "df5 = create_json_df(spark, get_json_data(2000000, 10, insert_dest))\n",
    "df5.count()\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df5.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000010"
     ]
    }
   ],
   "source": [
    "df6=spark.read.format(HUDI_FORMAT).load(config[\"target\"]+\"/*\")\n",
    "df6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-------------+\n",
      "|trip_id|route_id|tstamp             |destination  |\n",
      "+-------+--------+-------------------+-------------+\n",
      "|1999997|H       |2022-03-25 09:40:07|Philadelphia |\n",
      "|1999998|I       |2022-03-25 09:40:07|Miami        |\n",
      "|1999999|J       |2022-03-25 09:40:07|San Francisco|\n",
      "|2000009|J       |2022-03-25 09:45:31|Syracuse     |\n",
      "|2000008|I       |2022-03-25 09:45:31|Syracuse     |\n",
      "|2000007|H       |2022-03-25 09:45:31|Syracuse     |\n",
      "|2000006|G       |2022-03-25 09:45:31|Syracuse     |\n",
      "|2000001|B       |2022-03-25 09:45:31|Syracuse     |\n",
      "|2000000|A       |2022-03-25 09:45:31|Syracuse     |\n",
      "|2000005|F       |2022-03-25 09:45:31|Syracuse     |\n",
      "|2000004|E       |2022-03-25 09:45:31|Syracuse     |\n",
      "|2000003|D       |2022-03-25 09:45:31|Syracuse     |\n",
      "|2000002|C       |2022-03-25 09:45:31|Syracuse     |\n",
      "+-------+--------+-------------------+-------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id > 1999996\").show(50,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental View\n",
    "Hudi provides **Incremental** **View**. This is helpful to extract only the changes between any two commits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20220325094500"
     ]
    }
   ],
   "source": [
    "commits = list(map(lambda row: row[0], spark.sql(\"select distinct _hoodie_commit_time as commit_time from \"+config['table_name'] +\" order by commit_time\").limit(50).collect()))\n",
    "beginTime = commits[len(commits) - 2] # commit time we are interested in\n",
    "print(beginTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+--------+-------+-------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|destination|route_id|trip_id|             tstamp|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+--------+-------+-------------------+\n",
      "|     20220325094536| 20220325094536_0_11|           2000009|                      |319c4469-92d7-433...|   Syracuse|       J|2000009|2022-03-25 09:45:31|\n",
      "|     20220325094536| 20220325094536_0_12|           2000008|                      |319c4469-92d7-433...|   Syracuse|       I|2000008|2022-03-25 09:45:31|\n",
      "|     20220325094536| 20220325094536_0_13|           2000007|                      |319c4469-92d7-433...|   Syracuse|       H|2000007|2022-03-25 09:45:31|\n",
      "|     20220325094536| 20220325094536_0_14|           2000006|                      |319c4469-92d7-433...|   Syracuse|       G|2000006|2022-03-25 09:45:31|\n",
      "|     20220325094536| 20220325094536_0_15|           2000001|                      |319c4469-92d7-433...|   Syracuse|       B|2000001|2022-03-25 09:45:31|\n",
      "|     20220325094536| 20220325094536_0_16|           2000000|                      |319c4469-92d7-433...|   Syracuse|       A|2000000|2022-03-25 09:45:31|\n",
      "|     20220325094536| 20220325094536_0_17|           2000005|                      |319c4469-92d7-433...|   Syracuse|       F|2000005|2022-03-25 09:45:31|\n",
      "|     20220325094536| 20220325094536_0_18|           2000004|                      |319c4469-92d7-433...|   Syracuse|       E|2000004|2022-03-25 09:45:31|\n",
      "|     20220325094536| 20220325094536_0_19|           2000003|                      |319c4469-92d7-433...|   Syracuse|       D|2000003|2022-03-25 09:45:31|\n",
      "|     20220325094536| 20220325094536_0_20|           2000002|                      |319c4469-92d7-433...|   Syracuse|       C|2000002|2022-03-25 09:45:31|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+--------+-------+-------------------+"
     ]
    }
   ],
   "source": [
    "# We are taking second last commit_time as our begin time to find records that have been changed between last two commits\n",
    "\n",
    "incremental_read_options = {\n",
    "  VIEW_TYPE_OPT_KEY: VIEW_TYPE_INCREMENTAL_OPT_VAL,\n",
    "  BEGIN_INSTANTTIME_OPT_KEY: beginTime,\n",
    "}\n",
    "\n",
    "incQueryDF=spark.read.format(HUDI_FORMAT).options(**incremental_read_options).load(config[\"target\"]+\"/*\")\n",
    "incQueryDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that only newly added records from trip_id 2000000 to 2000009 are shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Records.\n",
    "\n",
    "Apache Hudi supports implementing two types of deletes on data stored in Hudi datasets, by enabling the user to specify a different record payload implementation.\n",
    "\n",
    "* **Soft Deletes** : With soft deletes, user wants to retain the key but just null out the values for all other fields. This can be simply achieved by ensuring the appropriate fields are nullable in the dataset schema and simply upserting the dataset after setting these fields to null.\n",
    "    \n",
    "* **Hard Deletes** : A stronger form of delete is to physically remove any trace of the record from the dataset. \n",
    "\n",
    "Let's now execute some hard delete operations on our dataset which will remove the records from our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delete the 10 records with the \"Syracuse\" destination we added to the table. You can hard delete data by setting OPERATION_OPT_KEY to DELETE_OPERATION_OPT_VAL to remove all records in the dataset you submit\n",
    "```\n",
    ".option(OPERATION_OPT_KEY,DELETE_OPERATION_OPT_VAL)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+-------------------+\n",
      "|destination|route_id|trip_id|             tstamp|\n",
      "+-----------+--------+-------+-------------------+\n",
      "|   Syracuse|       A|2000000|2022-03-25 09:45:31|\n",
      "|   Syracuse|       B|2000001|2022-03-25 09:45:31|\n",
      "|   Syracuse|       C|2000002|2022-03-25 09:45:31|\n",
      "|   Syracuse|       D|2000003|2022-03-25 09:45:31|\n",
      "|   Syracuse|       E|2000004|2022-03-25 09:45:31|\n",
      "|   Syracuse|       F|2000005|2022-03-25 09:45:31|\n",
      "|   Syracuse|       G|2000006|2022-03-25 09:45:31|\n",
      "|   Syracuse|       H|2000007|2022-03-25 09:45:31|\n",
      "|   Syracuse|       I|2000008|2022-03-25 09:45:31|\n",
      "|   Syracuse|       J|2000009|2022-03-25 09:45:31|\n",
      "+-----------+--------+-------+-------------------+"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df5.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, DELETE_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-------------+\n",
      "|trip_id|route_id|tstamp             |destination  |\n",
      "+-------+--------+-------------------+-------------+\n",
      "|1999997|H       |2022-03-25 09:40:07|Philadelphia |\n",
      "|1999998|I       |2022-03-25 09:40:07|Miami        |\n",
      "|1999999|J       |2022-03-25 09:40:07|San Francisco|\n",
      "+-------+--------+-------------------+-------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id > 1999996\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the records > 2000000 no longer exist in our table.\n",
    "\n",
    "Let's observe the number of files in S3. Expected : 6 files (initial files (3) + one upsert + one insert + one delete = 6)\n",
    "\n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://<Your S3 Bucket Here>/tmp/hudi/hudi_trips_table/ --summarize --human-readable\n",
    "                           PRE .hoodie/\n",
    "2021-11-15 19:47:08          0 .hoodie_$folder$\n",
    "2021-11-15 19:47:19         93 .hoodie_partition_metadata\n",
    "2021-11-15 19:47:29    4874333 2ac15cd9-60ce-4fbd-8ae7-91abae3a3a12-0_2-15-105_20211115194706.parquet\n",
    "2021-11-15 19:57:26    4936875 3de41050-e3d7-41d2-8ce5-2892ac24e2e1-0_0-164-14890_20211115195703.parquet\n",
    "2021-11-15 19:53:14    4936622 3de41050-e3d7-41d2-8ce5-2892ac24e2e1-0_0-99-692_20211115195304.parquet\n",
    "2021-11-15 19:47:29    4936875 3de41050-e3d7-41d2-8ce5-2892ac24e2e1-0_1-15-104_20211115194706.parquet\n",
    "2021-11-15 19:47:26    4672448 bfa09b26-b3af-4f1a-90d1-c6f71bf70a07-0_0-15-103_20211115194706.parquet\n",
    "2021-11-15 19:50:29    4672282 bfa09b26-b3af-4f1a-90d1-c6f71bf70a07-0_0-59-416_20211115195018.parquet\n",
    "\n",
    "Total Objects: 8\n",
    "   Total Size: 27.5 MiB\n",
    "\n",
    "```\n",
    "\n",
    "In our example, we set number of commits to retain as 4. So, maximum only 4 new versions of a single file can be created on top of our bulk insert files.  This also directly translates into how much data you can incrementally pull on this table, default is 10. Hudi Cleaner Policy will delete older files when writing based on the commit retain policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollback\n",
    "\n",
    "Let's say we want to roll back the last delete we made. \n",
    "\n",
    "Start the hudi CLI using the command-> \n",
    "    /usr/lib/hudi/cli/bin/hudi-cli.sh\n",
    "\n",
    "connect to the hudi/hudi_trips_table using the command ->  \n",
    "    connect --path s3://BUCKET_NAME/hudi/hudi_trips_table/\n",
    "\n",
    "List the commits using command ->> \n",
    "    commits show\n",
    "\n",
    "hudi:hudi_trips_table->commits show\n",
    "20/04/28 23:46:09 INFO s3n.S3NativeFileSystem: Opening 's3://<Your S3 Bucket Here>/tmp/hudi/hudi_trips_table/.hoodie/20200428231705.commit' for reading\n",
    "20/04/28 23:46:09 INFO s3n.S3NativeFileSystem: Opening 's3://<Your S3 Bucket Here>/tmp/hudi/hudi_trips_table/.hoodie/20200428231528.commit' for reading\n",
    "20/04/28 23:46:09 INFO s3n.S3NativeFileSystem: Opening 's3://<Your S3 Bucket Here>/tmp/hudi/hudi_trips_table/.hoodie/20200428231141.commit' for reading\n",
    "╔════════════════╤═════════════════════╤═══════════════════╤═════════════════════╤══════════════════════════╤═══════════════════════╤══════════════════════════════╤══════════════╗\n",
    "║ CommitTime     │ Total Bytes Written │ Total Files Added │ Total Files Updated │ Total Partitions Written │ Total Records Written │ Total Update Records Written │ Total Errors ║\n",
    "╠════════════════╪═════════════════════╪═══════════════════╪═════════════════════╪══════════════════════════╪═══════════════════════╪══════════════════════════════╪══════════════╣\n",
    "║ 20200428231814 │ 4.4 MB              │ 0                 │ 1                   │ 1                        │ 642132                │ 0                            │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20200428231528 │ 4.8 MB              │ 0                 │ 1                   │ 1                        │ 697329                │ 10                           │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20200428231528 │ 4.8 MB              │ 0                 │ 1                   │ 1                        │ 697329                │ 10                           │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "\n",
    "║ 20200428231141 │ 13.8 MB             │ 3                 │ 0                   │ 1                        │ 2000000               │ 0                            │ 0            ║\n",
    "╚════════════════╧═════════════════════╧═══════════════════╧═════════════════════╧══════════════════════════╧═══════════════════════╧══════════════════════════════╧══════════════╝\n",
    "\n",
    "commit rollback --commit 20200428231814\n",
    "    \n",
    "Now let us check what happened to the records we deleted earlier.\n",
    "\n",
    "If you see this message- \"Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\". Make sure you have enough resources to execute the rollback job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-------------+\n",
      "|trip_id|route_id|tstamp             |destination  |\n",
      "+-------+--------+-------------------+-------------+\n",
      "|1999997|H       |2022-03-25 09:40:07|Philadelphia |\n",
      "|1999998|I       |2022-03-25 09:40:07|Miami        |\n",
      "|1999999|J       |2022-03-25 09:40:07|San Francisco|\n",
      "|2000009|J       |2022-03-25 09:45:31|Syracuse     |\n",
      "|2000008|I       |2022-03-25 09:45:31|Syracuse     |\n",
      "|2000007|H       |2022-03-25 09:45:31|Syracuse     |\n",
      "|2000006|G       |2022-03-25 09:45:31|Syracuse     |\n",
      "|2000001|B       |2022-03-25 09:45:31|Syracuse     |\n",
      "|2000000|A       |2022-03-25 09:45:31|Syracuse     |\n",
      "|2000005|F       |2022-03-25 09:45:31|Syracuse     |\n",
      "|2000004|E       |2022-03-25 09:45:31|Syracuse     |\n",
      "|2000003|D       |2022-03-25 09:45:31|Syracuse     |\n",
      "|2000002|C       |2022-03-25 09:45:31|Syracuse     |\n",
      "+-------+--------+-------------------+-------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id > 1999996\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced:Understanding-Hudi-Commits\n",
    "\n",
    "After rollback you should have 5 files in your s3 bucket.Lets understand how does hudi clean older files. There are 2 properties that determine hudi cleaner behavior. \n",
    "\n",
    "**KEEP_LATEST_COMMITS:** This is the default policy. This is a temporal cleaning policy that ensures the effect of having lookback into all the changes that happened in the last X commits. Suppose a writer is ingesting data into a Hudi dataset every 30 minutes and the longest running query can take 5 hours to finish, then the user should retain atleast the last 10 commits. With such a configuration, we ensure that the oldest version of a file is kept on disk for at least 5 hours, thereby preventing the longest running query from failing at any point in time. Incremental cleaning is also possible using this policy.\n",
    "\n",
    "**KEEP_LATEST_FILE_VERSIONS:** This policy has the effect of keeping N number of file versions irrespective of time. This policy is useful when it is known how many MAX versions of the file does one want to keep at any given time. To achieve the same behaviour as before of preventing long running queries from failing, one should do their calculations based on data patterns. Alternatively, this policy is also useful if a user just wants to maintain 1 latest version of the file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run the upsert operation with these commit configuration and notice the change in behavior of files created on S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## KEEP_LATEST_COMMITS\n",
    "## Lets run the upsert statement again to create new version of the file. Keep the output of aws s3 ls handy prior to running below command to check the difference\n",
    "\n",
    "(df3.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,2)\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the file names before and after the upsert statement. Because of .option(HUDI_COMMITS_RETAINED,2) you will not have more than 2 versions of the single file. Older version gets cleaned up as new version is written. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## KEEP_LATEST_FILE_VERSIONS\n",
    "\n",
    "(df3.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_FILE_VERSIONS) \n",
    "      .option(HUDI_FILES_RETAINED,1)\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the change in files on S3 now. Because of .option(HUDI_FILES_RETAINED,1) you will only have 3 files (3 base * 1 version of each file). Notice the file names are still the same, however they might be versioned differently. \n",
    "\n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://<Your S3 Bucket Here>/tmp/hudi/hudi_trips_table/ --summarize --human-readable\n",
    "                           PRE .hoodie/\n",
    "2021-11-15 19:47:08          0 .hoodie_$folder$\n",
    "2021-11-15 19:47:19         93 .hoodie_partition_metadata\n",
    "2021-11-15 19:47:29    4874333 2ac15cd9-60ce-4fbd-8ae7-91abae3a3a12-0_2-15-105_20211115194706.parquet\n",
    "2021-11-15 19:53:14    4936622 3de41050-e3d7-41d2-8ce5-2892ac24e2e1-0_0-99-692_20211115195304.parquet\n",
    "2021-11-15 20:49:04    4672242 bfa09b26-b3af-4f1a-90d1-c6f71bf70a07-0_0-65-482_20211115204852.parquet\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
