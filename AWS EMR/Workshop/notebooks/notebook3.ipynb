{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab : Batch Updates to a S3 Datalake using Apache Hudi\n",
    "\n",
    "# Merge On Read\n",
    "\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "- 1. [Overview](#Overview)\n",
    "- 2. [Merge on Read](#Merge-On-Read) \n",
    "   - 2.1 [Bulk Insert the Initial Dataset](#Bulk-Insert-the-Initial-Dataset)\n",
    "   - 2.2 [Batch Upsert some records](#Batch-Upsert-some-records)\n",
    "   - 2.3 [Deleting Records](#Deleting-Records.)\n",
    "-3. [Hudi Compaction](#Hudi-Compaction)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates using PySpark on [Apache Hudi](https://aws.amazon.com/emr/features/hudi/) on Amazon EMR to insert/upsert/delete records to an S3 data lake.\n",
    "\n",
    "Here is a good reference link to read later:\n",
    "\n",
    "* [How Hudi Works](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-how-it-works.html)\n",
    "\n",
    "This notebook covers the following concepts when writing  Merge-On-Read tables to an S3 Datalake:\n",
    "\n",
    "- Write Hudi Spark jobs in PySpark.\n",
    "- Bulk Insert the Initial Dataset.\n",
    "- Sync the Hudi tables to the Hive/Glue Catalog.\n",
    "- Upsert some records to a Hudi MOR table.\n",
    "- Delete records from a Hudi MOR table.\n",
    "- Understand Hudi Compaction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This demo is based on Hudi version 0.8.0 and runs fine on Jupyter Notebooks connected to a 1 node (r5.4xlarge) EMR cluster with configuration listed below \n",
    "\n",
    " - EMR versions 6.5.0 \n",
    " \n",
    " - Software configuration\n",
    "\n",
    "       - Hadoop 3.2.1\n",
    "       - Hive 3.1.2\n",
    "       - Livy 0.7.1\n",
    "       - JupyterHub 1.4.1\n",
    "       - Spark 3.1.2\n",
    "       \n",
    "       \n",
    " - AWS Glue Data Catalog settings - Select the below listed check boxes\n",
    "       - Use for Hive table metadata  \n",
    "       - Use for Spark table metadata\n",
    "\n",
    "\n",
    "\n",
    "### Connect to the Master Node of EMR cluster Using SSH :\n",
    "    - ssh -i ~/xxxx.pem hadoop@<ec2-xx-xxx-xx-xx.us-west-2.compute.amazonaws.com>\n",
    "\n",
    "    - Ensure  the below listed files are copied into HDFS.\n",
    "\n",
    "    - hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar hdfs:///user/hadoop/\n",
    "\n",
    "    - hdfs dfs -copyFromLocal /usr/lib/spark/external/lib/spark-avro.jar hdfs:///user/hadoop/\n",
    "\n",
    "    - hdfs dfs -copyFromLocal /usr/share/aws/aws-java-sdk/aws-java-sdk-bundle-1.12.31.jar hdfs:///user/hadoop/\n",
    "      (https://github.com/apache/hudi/issues/5053)\n",
    "\n",
    "Let's start by initializing the Spark Session to connect this notebook to our Spark EMR cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///user/hadoop/aws-java-sdk-bundle-1.12.31.jar, hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar', 'spark.sql.hive.convertMetastoreParquet': 'false', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.dynamicAllocation.executorIdleTimeout': 3600, 'spark.executor.memory': '5G', 'spark.executor.cores': 4, 'spark.dynamicAllocation.initialExecutors': 5}, 'proxyUser': 'user_sekar', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars\":\"hdfs:///user/hadoop/aws-java-sdk-bundle-1.12.31.jar, hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar\",\n",
    "             \"spark.sql.hive.convertMetastoreParquet\":\"false\",     \n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "             \"spark.dynamicAllocation.executorIdleTimeout\": 3600,\n",
    "             \"spark.executor.memory\": \"5G\",\n",
    "             \"spark.executor.cores\": 4,\n",
    "             \"spark.dynamicAllocation.initialExecutors\":5\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constants for Python to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02959ddc24004eff9ab4387bf62ccfeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>10</td><td>application_1648194189527_0012</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-77-205.ec2.internal:20888/proxy/application_1648194189527_0012/\" class=\"emr-proxy-link\" emr-resource=\"j-3CTHLLOHA0RVG\n",
       "\" application-id=\"application_1648194189527_0012\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-77-205.ec2.internal:8042/node/containerlogs/container_1648194189527_0012_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General Constants\n",
    "HUDI_FORMAT = \"org.apache.hudi\"\n",
    "TABLE_NAME = \"hoodie.table.name\"\n",
    "RECORDKEY_FIELD_OPT_KEY = \"hoodie.datasource.write.recordkey.field\"\n",
    "PRECOMBINE_FIELD_OPT_KEY = \"hoodie.datasource.write.precombine.field\"\n",
    "OPERATION_OPT_KEY = \"hoodie.datasource.write.operation\"\n",
    "BULK_INSERT_OPERATION_OPT_VAL = \"bulk_insert\"\n",
    "UPSERT_OPERATION_OPT_VAL = \"upsert\"\n",
    "DELETE_OPERATION_OPT_VAL = \"delete\"\n",
    "BULK_INSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\"\n",
    "UPSERT_PARALLELISM = \"hoodie.upsert.shuffle.parallelism\"\n",
    "S3_CONSISTENCY_CHECK = \"hoodie.consistency.check.enabled\"\n",
    "HUDI_CLEANER_POLICY = \"hoodie.cleaner.policy\"\n",
    "KEEP_LATEST_COMMITS = \"KEEP_LATEST_COMMITS\"\n",
    "HUDI_COMMITS_RETAINED = \"hoodie.cleaner.commits.retained\"\n",
    "PAYLOAD_CLASS_OPT_KEY = \"hoodie.datasource.write.payload.class.key()\"\n",
    "EMPTY_PAYLOAD_CLASS_OPT_VAL = \"org.apache.hudi.EmptyHoodieRecordPayload\"\n",
    "\n",
    "# Hive Constants\n",
    "HIVE_SYNC_ENABLED_OPT_KEY=\"hoodie.datasource.hive_sync.enable\"\n",
    "HIVE_PARTITION_FIELDS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_fields\"\n",
    "HIVE_ASSUME_DATE_PARTITION_OPT_KEY=\"hoodie.datasource.hive_sync.assume_date_partitioning\"\n",
    "HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_extractor_class\"\n",
    "HIVE_TABLE_OPT_KEY=\"hoodie.datasource.hive_sync.table\"\n",
    "\n",
    "# Partition Constants\n",
    "NONPARTITION_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.NonPartitionedExtractor\"\n",
    "MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.MultiPartKeysValueExtractor\"\n",
    "KEYGENERATOR_CLASS_OPT_KEY=\"hoodie.datasource.write.keygenerator.class\"\n",
    "NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.keygen.NonpartitionedKeyGenerator\"\n",
    "COMPLEX_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.ComplexKeyGenerator\"\n",
    "PARTITIONPATH_FIELD_OPT_KEY=\"hoodie.datasource.write.partitionpath.field\"\n",
    "\n",
    "#Incremental Constants\n",
    "VIEW_TYPE_OPT_KEY=\"hoodie.datasource.view.type\"\n",
    "BEGIN_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.begin.instanttime\"\n",
    "VIEW_TYPE_INCREMENTAL_OPT_VAL=\"incremental\"\n",
    "END_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.end.instanttime\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to create JSON data and Spark dataframe from this data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge On Read \n",
    "\n",
    "For near real-time applications that mandate quick upserts, MERGE_ON_READ table type would be better suited. MOR table stores incoming upserts for each file group, onto a row based delta log (In Avro file format). This log is then merged with the existing Parquet file using a compactor during reads. \n",
    "\n",
    "Make sure you update the bucket name to your unique bucket before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e5733e013043b4acc4ebbb470cc994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## CHANGE ME ##\n",
    "config = {\n",
    "    \"table_name\": \"hudi_trips_table_mor\",\n",
    "    \"target\": \"s3://<Your S3 Bucket Here>/hudi/hudi_trips_table_mor/\",\n",
    "    \"primary_key\": \"trip_id\",\n",
    "    \"sort_key\": \"tstamp\",\n",
    "    \"commits_to_retain\": \"3\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a087581f7c584439997f3977c42e91f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "STORAGE_TYPE_OPT_KEY=\"hoodie.datasource.write.storage.type\"\n",
    "COMPACTION_INLINE_OPT_KEY=\"hoodie.compact.inline\"\n",
    "COMPACTION_MAX_DELTA_COMMITS_OPT_KEY=\"hoodie.compact.inline.max.delta.commits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814f3e98ae61415e9c874a9e08a2a2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generates Data\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_json_data(start, count, dest):\n",
    "    time_stamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    data = [{\"trip_id\": i, \"tstamp\": time_stamp, \"route_id\": chr(65 + (i % 10)), \"destination\": dest[i%10]} for i in range(start, start + count)]\n",
    "    return data\n",
    "\n",
    "# Creates the Dataframe\n",
    "def create_json_df(spark, data):\n",
    "    sc = spark.sparkContext\n",
    "    return spark.read.json(sc.parallelize(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's generate 2M records to load into our Data Lake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df06a69b1e0a4231987b901fd918574b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+-------+-------------------+\n",
      "|  destination|route_id|trip_id|             tstamp|\n",
      "+-------------+--------+-------+-------------------+\n",
      "|      Seattle|       A|      0|2022-03-27 07:04:39|\n",
      "|     New York|       B|      1|2022-03-27 07:04:39|\n",
      "|   New Jersey|       C|      2|2022-03-27 07:04:39|\n",
      "|  Los Angeles|       D|      3|2022-03-27 07:04:39|\n",
      "|    Las Vegas|       E|      4|2022-03-27 07:04:39|\n",
      "|       Tucson|       F|      5|2022-03-27 07:04:39|\n",
      "|Washington DC|       G|      6|2022-03-27 07:04:39|\n",
      "| Philadelphia|       H|      7|2022-03-27 07:04:39|\n",
      "|        Miami|       I|      8|2022-03-27 07:04:39|\n",
      "|San Francisco|       J|      9|2022-03-27 07:04:39|\n",
      "|      Seattle|       A|     10|2022-03-27 07:04:39|\n",
      "|     New York|       B|     11|2022-03-27 07:04:39|\n",
      "|   New Jersey|       C|     12|2022-03-27 07:04:39|\n",
      "|  Los Angeles|       D|     13|2022-03-27 07:04:39|\n",
      "|    Las Vegas|       E|     14|2022-03-27 07:04:39|\n",
      "|       Tucson|       F|     15|2022-03-27 07:04:39|\n",
      "|Washington DC|       G|     16|2022-03-27 07:04:39|\n",
      "| Philadelphia|       H|     17|2022-03-27 07:04:39|\n",
      "|        Miami|       I|     18|2022-03-27 07:04:39|\n",
      "|San Francisco|       J|     19|2022-03-27 07:04:39|\n",
      "+-------------+--------+-------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "mor_dest = [\"Seattle\", \"New York\", \"New Jersey\", \"Los Angeles\", \"Las Vegas\", \"Tucson\",\"Washington DC\",\"Philadelphia\",\"Miami\",\"San Francisco\"]\n",
    "df2 = create_json_df(spark, get_json_data(0, 2000000, mor_dest))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk Insert the Initial Dataset\n",
    "\n",
    "We will be using the Merge on Read storage option(STORAGE_TYPE_OPT_KEY, \"MERGE_ON_READ\") which  is to be explicitly set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3e399fd9a245849c8e0f48165e0a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df2.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 3)\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .option(STORAGE_TYPE_OPT_KEY, \"MERGE_ON_READ\")\n",
    "      .option(COMPACTION_INLINE_OPT_KEY, \"false\")\n",
    "      .option(COMPACTION_MAX_DELTA_COMMITS_OPT_KEY, \"3\")\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the number of files \n",
    "\n",
    "Let us check the contents of S3 path. Bulk insert operation on Copy-On-Write and Merge-On-Read tables is identical in terms of performance. \n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://< S3 Bucket >/hudi/hudi_mor_trips_table/\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:30:20          0 .hoodie_$folder$\n",
    "2020-04-28 23:30:26         93 .hoodie_partition_metadata\n",
    "2020-04-28 23:30:33    4378000 45b1ce07-f9ac-496d-8b03-20af011a0c44-0_1-194-3566_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5048941 932d5e97-c5f0-4c91-a7f6-f65d487a5e2b-0_2-194-3567_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5065824 ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_0-194-3565_20200428233020.parquet\n",
    "```\n",
    "\n",
    "Notice the delta commits \n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://< S3 Bucket >/hudi/hudi_mor_trips_table/.hoodie/\n",
    "2020-04-28 23:30:21          0 .aux_$folder$\n",
    "2020-04-28 23:30:21          0 .temp_$folder$\n",
    "2020-04-28 23:30:37       1077 20200428233020.clean\n",
    "2020-04-28 23:30:36       4929 20200428233020.deltacommit\n",
    "2020-04-28 23:30:21          0 archived_$folder$\n",
    "2020-04-28 23:30:21        264 hoodie.properties\n",
    "```\n",
    "\n",
    "This is the first commit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Upsert some records\n",
    "\n",
    "\n",
    "Now let us try to upsert some records into MOR table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e5dc6c9d42480e8d9cf31e26cf69a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+-------------------+\n",
      "|destination|route_id|trip_id|             tstamp|\n",
      "+-----------+--------+-------+-------------------+\n",
      "|  San Diego|       A|1000000|2022-03-27 07:05:58|\n",
      "|  San Diego|       B|1000001|2022-03-27 07:05:58|\n",
      "|  San Diego|       C|1000002|2022-03-27 07:05:58|\n",
      "|  San Diego|       D|1000003|2022-03-27 07:05:58|\n",
      "|  San Diego|       E|1000004|2022-03-27 07:05:58|\n",
      "|  San Diego|       F|1000005|2022-03-27 07:05:58|\n",
      "|  San Diego|       G|1000006|2022-03-27 07:05:58|\n",
      "|  San Diego|       H|1000007|2022-03-27 07:05:58|\n",
      "|  San Diego|       I|1000008|2022-03-27 07:05:58|\n",
      "|  San Diego|       J|1000009|2022-03-27 07:05:58|\n",
      "+-----------+--------+-------+-------------------+"
     ]
    }
   ],
   "source": [
    "upsert_dest = [\"San Diego\", \"San Diego\", \"San Diego\", \"San Diego\", \"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\"]\n",
    "df3 = create_json_df(spark, get_json_data(1000000, 10, upsert_dest))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a750685229c84a46a2afd61d7c0e93c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df3.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .option(STORAGE_TYPE_OPT_KEY, \"MERGE_ON_READ\")\n",
    "      .option(COMPACTION_INLINE_OPT_KEY, \"false\")\n",
    "      .option(COMPACTION_MAX_DELTA_COMMITS_OPT_KEY, \"3\")\n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the number of tables created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c6cdcafca94278afbaf858715ebcb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------------+-----------+\n",
      "|database|tableName                     |isTemporary|\n",
      "+--------+------------------------------+-----------+\n",
      "|default |hudi_partitioned_trips_table  |false      |\n",
      "|default |hudi_trips_table_mor_ro       |false      |\n",
      "|default |hudi_trips_table_mor_rt       |false      |\n",
      "|default |hudi_trips_table_v2           |false      |\n",
      "|default |sales_order_detail_hudi_cow   |false      |\n",
      "|default |sales_order_detail_hudi_mor_ro|false      |\n",
      "|default |sales_order_detail_hudi_mor_rt|false      |\n",
      "+--------+------------------------------+-----------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can observe that there are two tables _ro and _rt created for  Merge On Read storage option. \n",
    "\n",
    "1.hudi_mor_trips_table_ro     \n",
    "2.hudi_mor_trips_table_rt  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read optimized view** provides the latest compacted dataset from MoR tables.\n",
    "Lets query the table  -> hudi_mor_trips_table_ro . Since the table is not yet compacted, upserted records will not be reflected in _ro table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529219f6f4104ee1bd8094184d0bfea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+-------------------+\n",
      "|trip_id|route_id|destination  |tstamp             |\n",
      "+-------+--------+-------------+-------------------+\n",
      "|1000000|A       |Seattle      |2022-03-27 07:04:39|\n",
      "|1000001|B       |New York     |2022-03-27 07:04:39|\n",
      "|1000002|C       |New Jersey   |2022-03-27 07:04:39|\n",
      "|1000003|D       |Los Angeles  |2022-03-27 07:04:39|\n",
      "|1000004|E       |Las Vegas    |2022-03-27 07:04:39|\n",
      "|1000005|F       |Tucson       |2022-03-27 07:04:39|\n",
      "|1000006|G       |Washington DC|2022-03-27 07:04:39|\n",
      "|1000007|H       |Philadelphia |2022-03-27 07:04:39|\n",
      "|1000008|I       |Miami        |2022-03-27 07:04:39|\n",
      "|1000009|J       |San Francisco|2022-03-27 07:04:39|\n",
      "|1000010|A       |Seattle      |2022-03-27 07:04:39|\n",
      "|999996 |G       |Washington DC|2022-03-27 07:04:39|\n",
      "|999997 |H       |Philadelphia |2022-03-27 07:04:39|\n",
      "|999998 |I       |Miami        |2022-03-27 07:04:39|\n",
      "|999999 |J       |San Francisco|2022-03-27 07:04:39|\n",
      "+-------+--------+-------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name']+\"_ro where trip_id between 999996 and 1000010\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real time view** provides the latest committed data from a MoR table by merging the columnar and row-based files inline.\n",
    "Lets query the real-time table -> hudi_mor_trips_table_rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfacf94aa140400b8ebb7f4e75f10d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+-------------------+\n",
      "|trip_id|route_id|destination  |tstamp             |\n",
      "+-------+--------+-------------+-------------------+\n",
      "|1000000|A       |San Diego    |2022-03-27 07:05:58|\n",
      "|1000001|B       |San Diego    |2022-03-27 07:05:58|\n",
      "|1000002|C       |San Diego    |2022-03-27 07:05:58|\n",
      "|1000003|D       |San Diego    |2022-03-27 07:05:58|\n",
      "|1000004|E       |San Diego    |2022-03-27 07:05:58|\n",
      "|1000005|F       |San Diego    |2022-03-27 07:05:58|\n",
      "|1000006|G       |San Diego    |2022-03-27 07:05:58|\n",
      "|1000007|H       |San Diego    |2022-03-27 07:05:58|\n",
      "|1000008|I       |San Diego    |2022-03-27 07:05:58|\n",
      "|1000009|J       |San Diego    |2022-03-27 07:05:58|\n",
      "|1000010|A       |Seattle      |2022-03-27 07:04:39|\n",
      "|999996 |G       |Washington DC|2022-03-27 07:04:39|\n",
      "|999997 |H       |Philadelphia |2022-03-27 07:04:39|\n",
      "|999998 |I       |Miami        |2022-03-27 07:04:39|\n",
      "|999999 |J       |San Francisco|2022-03-27 07:04:39|\n",
      "+-------+--------+-------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name']+\"_rt where trip_id between 999996 and 1000010\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the S3 path again. There is no change in number of Parquet files after upsert operation unlike Copy-On-Write tables. However, you will notice a new delta file created for the upsert operation\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<Your S3 Bucket Here>/hudi/hudi_trips_table_mor/\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:33:22       2071 .ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_20200428233020.log.1_0-227-3837\n",
    "2020-04-28 23:30:20          0 .hoodie_$folder$\n",
    "2020-04-28 23:30:26         93 .hoodie_partition_metadata\n",
    "2020-04-28 23:30:33    4378000 45b1ce07-f9ac-496d-8b03-20af011a0c44-0_1-194-3566_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5048941 932d5e97-c5f0-4c91-a7f6-f65d487a5e2b-0_2-194-3567_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5065824 ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_0-194-3565_20200428233020.parquet\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Records.\n",
    "\n",
    "Apache Hudi supports implementing two types of deletes on data stored in Hudi datasets, by enabling the user to specify a different record payload implementation.\n",
    "\n",
    "* **Soft Deletes** : With soft deletes, user wants to retain the key but just null out the values for all other fields. This can be simply achieved by ensuring the appropriate fields are nullable in the dataset schema and simply upserting the dataset after setting these fields to null.\n",
    "    \n",
    "* **Hard Deletes** : A stronger form of delete is to physically remove any trace of the record from the dataset. \n",
    "\n",
    "Let's now execute some hard delete operations on our dataset which will remove the records from our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delete the 10 records with the \"San Diego\" destination we upserted to the table. Note that the only change is the single line that set the OPERATION_OPT_KEY to DELETE_OPERATION_OPT_VAL to remove all records in the dataset you submit\n",
    "\n",
    "```\n",
    ".option(OPERATION_OPT_KEY, DELETE_OPERATION_OPT_VAL)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a0949ded964c0c950bda064163e3e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df3.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, DELETE_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .option(STORAGE_TYPE_OPT_KEY, \"MERGE_ON_READ\")\n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Query and observe that the updated records are reflected in only _rt table as it gives the most lastest snapshot of data. While, _ro table still contains the deleted rows.\n",
    "\n",
    "1.hudi_mor_trips_table_ro    \n",
    "2.hudi_mor_trips_table_rt  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437b16d5f613472894aae62e4639e68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+-------------------+\n",
      "|trip_id|route_id|destination  |tstamp             |\n",
      "+-------+--------+-------------+-------------------+\n",
      "|1000000|A       |Seattle      |2022-03-27 07:04:39|\n",
      "|1000001|B       |New York     |2022-03-27 07:04:39|\n",
      "|1000002|C       |New Jersey   |2022-03-27 07:04:39|\n",
      "|1000003|D       |Los Angeles  |2022-03-27 07:04:39|\n",
      "|1000004|E       |Las Vegas    |2022-03-27 07:04:39|\n",
      "|1000005|F       |Tucson       |2022-03-27 07:04:39|\n",
      "|1000006|G       |Washington DC|2022-03-27 07:04:39|\n",
      "|1000007|H       |Philadelphia |2022-03-27 07:04:39|\n",
      "|1000008|I       |Miami        |2022-03-27 07:04:39|\n",
      "|1000009|J       |San Francisco|2022-03-27 07:04:39|\n",
      "|1000010|A       |Seattle      |2022-03-27 07:04:39|\n",
      "|999996 |G       |Washington DC|2022-03-27 07:04:39|\n",
      "|999997 |H       |Philadelphia |2022-03-27 07:04:39|\n",
      "|999998 |I       |Miami        |2022-03-27 07:04:39|\n",
      "|999999 |J       |San Francisco|2022-03-27 07:04:39|\n",
      "+-------+--------+-------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name'] +\"_ro where trip_id between 999996 and 1000010\").show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+-------------------+\n",
      "|trip_id|route_id|destination  |tstamp             |\n",
      "+-------+--------+-------------+-------------------+\n",
      "|1000010|A       |Seattle      |2022-03-27 07:04:39|\n",
      "|999996 |G       |Washington DC|2022-03-27 07:04:39|\n",
      "|999997 |H       |Philadelphia |2022-03-27 07:04:39|\n",
      "|999998 |I       |Miami        |2022-03-27 07:04:39|\n",
      "|999999 |J       |San Francisco|2022-03-27 07:04:39|\n",
      "+-------+--------+-------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name']+\"_rt where trip_id between 999996 and 1000010\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Check the count of records in the two tables after deletion. \n",
    "\n",
    "1.hudi_mor_trips_table_ro     \n",
    "2.hudi_mor_trips_table_rt  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that total record count indicates that 10 upserted records have been deleted only from _rt table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cc58ba23394bbe9606de4f14ac37e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|2000000 |\n",
      "+--------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from \"+config['table_name']+\"_ro\" ).show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0309fab6364b4524bcf526fcea7d33d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|1999990 |\n",
      "+--------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from \"+config['table_name']+\"_rt\" ).show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hudi Compaction\n",
    "\n",
    "Compaction refers to background activity to reconcile differential data structures within Hudi e.g: moving updates from row based log files to columnar formats. Internally, compaction manifests as a special commit on the timeline. With Merge_On_Read Table, Hudi ingestion needs to also take care of compacting delta files. Compaction can be performed in an asynchronous-mode by letting compaction run concurrently with ingestion or in a serial fashion with one after another.\n",
    "\n",
    "- **Inline compaction:** For Merge On Read table types, inline compaction is turned on by default which runs after every ingestion. The compaction frequency can be changed by setting the property **hoodie.compact.inline.max.delta.commits**\n",
    "\n",
    "- **Manual Compaction:** Inline compaction works well for most of the use cases. However, there can be few latency sensitive cases, where you would want to disable inline compaction and run it manually.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now see how to schedule and run manual compaction on hudi table and how that impacts read optimized tables.Before we start, we need to have avro schema file. Copy and paste below schema on a notepad and save the file as *morHudiSchema.avsc* . Now place *morHudiSchema.avsc* to an appropriate S3 bucket, on which you have read and write access."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{\"type\":\"record\",\"name\":\"mor_table_record\",\"namespace\":\"hudi_mor_trips_table\",\"fields\":[{\"name\":\"destination\",\"type\":[\"string\",\"null\"]},{\"name\":\"route_id\",\"type\":[\"string\",\"null\"]},{\"name\":\"trip_id\",\"type\":[\"long\",\"null\"]},{\"name\":\"tstamp\",\"type\":[\"string\",\"null\"]}]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets open Hudi cli and schedule a manual compaction \n",
    "\n",
    "/usr/lib/hudi/cli/bin/hudi-cli.sh\n",
    "\n",
    "hudi->connect --path s3://< S3 Bucket >/hudi/hudi_trips_table_mor/\n",
    "\n",
    "hudi:hudi_trips_table_mor->compaction schedule --hoodieConfigs hoodie.compact.inline.max.delta.commits=1\n",
    "\n",
    "hudi:hudi_trips_table_mor->compactions show all\n",
    "\n",
    "|  Compaction Instant Time | State     | Total FileIds to be Compacted |\n",
    "| -------------------------| ----------|-------------------------------|\n",
    "| 20211110000358           | REQUESTED | 1                             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have scheduled a compaction. Lets go ahead and run the compaction job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refresh the metadata by running connect command again\n",
    "\n",
    "hudi->connect --path s3://< S3 Bucket >/hudi/hudi_trips_table_mor/\n",
    "\n",
    "hudi:hudi_trips_table_mor->compaction run --parallelism 2 --schemaFilePath s3://your-s3-bucket/morHudiSchema.avsc\n",
    "\n",
    "hudi:hudi_trips_table_mor->compactions show all\n",
    "\n",
    "|  Compaction Instant Time | State     | Total FileIds to be Compacted |\n",
    "| -------------------------| ----------|-------------------------------|\n",
    "| 20211110000358           | COMPLETED | 1                             |\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:- If you do not see state as COMPLETED, run the connect --path again to refresh the metadata.\n",
    "\n",
    "\n",
    "Notice the difference in count after we have run the compaction job. Both _ro and _rt tables have now latest snapshot of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47499e236c8d4b2eabb05db278d66bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|1999990 |\n",
      "+--------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from \"+config['table_name']+\"_ro\" ).show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a304e1ca86d840beb4e2916fbc2e72be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|1999990 |\n",
      "+--------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from \"+config['table_name']+\"_rt\" ).show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b56a36b768418595b727715b8e8575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+-------------------+\n",
      "|trip_id|route_id|destination  |tstamp             |\n",
      "+-------+--------+-------------+-------------------+\n",
      "|1000010|A       |Seattle      |2022-03-27 07:04:39|\n",
      "|999996 |G       |Washington DC|2022-03-27 07:04:39|\n",
      "|999997 |H       |Philadelphia |2022-03-27 07:04:39|\n",
      "|999998 |I       |Miami        |2022-03-27 07:04:39|\n",
      "|999999 |J       |San Francisco|2022-03-27 07:04:39|\n",
      "+-------+--------+-------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name']+\"_ro where trip_id between 999996 and 1000010\").show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+-------------------+\n",
      "|trip_id|route_id|destination  |tstamp             |\n",
      "+-------+--------+-------------+-------------------+\n",
      "|1000010|A       |Seattle      |2022-03-27 07:04:39|\n",
      "|999996 |G       |Washington DC|2022-03-27 07:04:39|\n",
      "|999997 |H       |Philadelphia |2022-03-27 07:04:39|\n",
      "|999998 |I       |Miami        |2022-03-27 07:04:39|\n",
      "|999999 |J       |San Francisco|2022-03-27 07:04:39|\n",
      "+-------+--------+-------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name']+\"_rt where trip_id between 999996 and 1000010\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, you can run the same queries on Athena."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
