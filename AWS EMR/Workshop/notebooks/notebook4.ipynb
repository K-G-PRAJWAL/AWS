{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab : Write a MultiKey Partitioned table to a S3 Datalake using Apache Hudi\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "2. [Working with Partitioned Tables](#Working-with-Partitioned-Tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates using PySpark on [Apache Hudi](https://aws.amazon.com/emr/features/hudi/) on Amazon EMR to Write a MultiKey Partitioned table records to an S3 data lake.\n",
    "\n",
    "\n",
    "This notebook covers the following concepts when writing Copy-On-Write tables to an S3 Datalake:\n",
    "\n",
    "- Write a MultiKey Partitioned table \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This demo is based on Hudi version 0.8.0 and runs fine on Jupyter Notebooks connected to a 1 node (r5.4xlarge) EMR cluster with configuration listed below \n",
    "\n",
    " - EMR versions 6.5.0 \n",
    " \n",
    " - Software configuration\n",
    "\n",
    "       - Hadoop 3.2.1\n",
    "       - Hive 3.1.2\n",
    "       - Livy 0.7.1\n",
    "       - JupyterHub 1.4.1\n",
    "       - Spark 3.1.2\n",
    "       \n",
    "       \n",
    " - AWS Glue Data Catalog settings - Select the below listed check boxes\n",
    "       - Use for Hive table metadata  \n",
    "       - Use for Spark table metadata\n",
    "\n",
    "\n",
    "\n",
    "### Connect to the Master Node of EMR cluster Using SSH :\n",
    "    - ssh -i ~/xxxx.pem hadoop@<ec2-xx-xxx-xx-xx.us-west-2.compute.amazonaws.com>\n",
    "\n",
    "    - Ensure  the below listed files are copied into HDFS.\n",
    "\n",
    "    - hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar hdfs:///user/hadoop/\n",
    "\n",
    "    - hdfs dfs -copyFromLocal /usr/lib/spark/external/lib/spark-avro.jar hdfs:///user/hadoop/\n",
    "\n",
    "    - hdfs dfs -copyFromLocal /usr/share/aws/aws-java-sdk/aws-java-sdk-bundle-1.12.31.jar hdfs:///user/hadoop/\n",
    "      (https://github.com/apache/hudi/issues/5053)\n",
    "\n",
    "Let's start by initializing the Spark Session to connect this notebook to our Spark EMR cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///user/hadoop/aws-java-sdk-bundle-1.12.31.jar, hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar', 'spark.sql.hive.convertMetastoreParquet': 'false', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.dynamicAllocation.executorIdleTimeout': 3600, 'spark.executor.memory': '5G', 'spark.executor.cores': 4, 'spark.dynamicAllocation.initialExecutors': 5}, 'proxyUser': 'user_sekar', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>8</td><td>application_1648194189527_0010</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-77-205.ec2.internal:20888/proxy/application_1648194189527_0010/\" class=\"emr-proxy-link\" emr-resource=\"j-3CTHLLOHA0RVG\n",
       "\" application-id=\"application_1648194189527_0010\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-77-205.ec2.internal:8042/node/containerlogs/container_1648194189527_0010_01_000001/livy\" >Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars\":\"hdfs:///user/hadoop/aws-java-sdk-bundle-1.12.31.jar, hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar\",\n",
    "             \"spark.sql.hive.convertMetastoreParquet\":\"false\",     \n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "             \"spark.dynamicAllocation.executorIdleTimeout\": 3600,\n",
    "             \"spark.executor.memory\": \"5G\",\n",
    "             \"spark.executor.cores\": 4,\n",
    "             \"spark.dynamicAllocation.initialExecutors\":5\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constants for Python to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08638249a4b0431fb94d7d1467144016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>9</td><td>application_1648194189527_0011</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-77-205.ec2.internal:20888/proxy/application_1648194189527_0011/\" class=\"emr-proxy-link\" emr-resource=\"j-3CTHLLOHA0RVG\n",
       "\" application-id=\"application_1648194189527_0011\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-77-205.ec2.internal:8042/node/containerlogs/container_1648194189527_0011_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General Constants\n",
    "HUDI_FORMAT = \"org.apache.hudi\"\n",
    "TABLE_NAME = \"hoodie.table.name\"\n",
    "RECORDKEY_FIELD_OPT_KEY = \"hoodie.datasource.write.recordkey.field\"\n",
    "PRECOMBINE_FIELD_OPT_KEY = \"hoodie.datasource.write.precombine.field\"\n",
    "OPERATION_OPT_KEY = \"hoodie.datasource.write.operation\"\n",
    "BULK_INSERT_OPERATION_OPT_VAL = \"bulk_insert\"\n",
    "UPSERT_OPERATION_OPT_VAL = \"upsert\"\n",
    "BULK_INSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\"\n",
    "UPSERT_PARALLELISM = \"hoodie.upsert.shuffle.parallelism\"\n",
    "S3_CONSISTENCY_CHECK = \"hoodie.consistency.check.enabled\"\n",
    "HUDI_CLEANER_POLICY = \"hoodie.cleaner.policy\"\n",
    "KEEP_LATEST_COMMITS = \"KEEP_LATEST_COMMITS\"\n",
    "HUDI_COMMITS_RETAINED = \"hoodie.cleaner.commits.retained\"\n",
    "PAYLOAD_CLASS_OPT_KEY = \"hoodie.datasource.write.payload.class\"\n",
    "EMPTY_PAYLOAD_CLASS_OPT_VAL = \"org.apache.hudi.EmptyHoodieRecordPayload\"\n",
    "\n",
    "# Hive Constants\n",
    "HIVE_SYNC_ENABLED_OPT_KEY=\"hoodie.datasource.hive_sync.enable\"\n",
    "HIVE_PARTITION_FIELDS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_fields\"\n",
    "HIVE_ASSUME_DATE_PARTITION_OPT_KEY=\"hoodie.datasource.hive_sync.assume_date_partitioning\"\n",
    "HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_extractor_class\"\n",
    "HIVE_TABLE_OPT_KEY=\"hoodie.datasource.hive_sync.table\"\n",
    "\n",
    "# Partition Constants\n",
    "NONPARTITION_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.NonPartitionedExtractor\"\n",
    "MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.MultiPartKeysValueExtractor\"\n",
    "KEYGENERATOR_CLASS_OPT_KEY=\"hoodie.datasource.write.keygenerator.class\"\n",
    "NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.NonpartitionedKeyGenerator\"\n",
    "COMPLEX_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.ComplexKeyGenerator\"\n",
    "PARTITIONPATH_FIELD_OPT_KEY=\"hoodie.datasource.write.partitionpath.field\"\n",
    "CUSTOM_KEY_GENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.keygen.CustomKeyGenerator\"\n",
    "\n",
    "#Incremental Constants\n",
    "VIEW_TYPE_OPT_KEY=\"hoodie.datasource.view.type\"\n",
    "BEGIN_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.begin.instanttime\"\n",
    "VIEW_TYPE_INCREMENTAL_OPT_VAL=\"incremental\"\n",
    "END_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.end.instanttime\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to create JSON data and Spark dataframe from this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c80d9eb47b1429faba9579090695828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generates Data\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_json_data(start, count, dest):\n",
    "    time_stamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    data = [{\"trip_id\": i, \"tstamp\": time_stamp, \"route_id\": chr(65 + (i % 10)), \"destination\": dest[i%10]} for i in range(start, start + count)]\n",
    "    return data\n",
    "\n",
    "# Creates the Dataframe\n",
    "def create_json_df(spark, data):\n",
    "    sc = spark.sparkContext\n",
    "    return spark.read.json(sc.parallelize(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Partitioned Tables\n",
    "\n",
    "Let's do the same thing with Partitioned Tables. For the sake of this demo, we will be making route_id and timestamp(yyyy-mm-dd) as partition fields. You can also have a nested partition structure like yyyy/mm/dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2ad7dc3fcf439e827da1eb31b9e0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## CHANGE ME ##\n",
    "config = {\n",
    "    \"table_name\": \"hudi_partitioned_trips_table\",\n",
    "    \"target\": \"s3://<Your S3 Bucket Here>/hudi/hudi_partitioned_trips_table\",\n",
    "    \"primary_key\": \"trip_id\",\n",
    "    \"sort_key\": \"tstamp\",\n",
    "    \"commits_to_retain\": \"2\",\n",
    "    \"partition_keys\" : \"routeid,tstamp\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75964c6f85c140829797af3003ddd551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+-------------------+\n",
      "|destination|route_id|trip_id|tstamp             |\n",
      "+-----------+--------+-------+-------------------+\n",
      "|Seattle    |A       |0      |2022-03-26 03:18:48|\n",
      "|New York   |B       |1      |2022-03-26 03:18:48|\n",
      "|New Jersey |C       |2      |2022-03-26 03:18:48|\n",
      "|Los Angeles|D       |3      |2022-03-26 03:18:48|\n",
      "|Las Vegas  |E       |4      |2022-03-26 03:18:48|\n",
      "+-----------+--------+-------+-------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "part_dest = [\"Seattle\", \"New York\", \"New Jersey\", \"Los Angeles\", \"Las Vegas\", \"Tucson\",\"Washington DC\",\"Philadelphia\",\"Miami\",\"San Francisco\"]\n",
    "df1 = create_json_df(spark, get_json_data(0, 2000000, part_dest))\n",
    "df1.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now write out the data to S3. Hudi provides various [key generators](https://hudi.apache.org/docs/writing_data#key-generation). We will be using CustomKeyGenerator which can take two possible values - SIMPLE and TIMESTAMP.  When using timestamp you will also need to provide supporting configs for [Key Generators](https://hudi.apache.org/blog/2021/02/13/hudi-key-generators/#key-generators) depending upon your input type. \n",
    "\n",
    "```\n",
    "      .option(HIVE_PARTITION_FIELDS_OPT_KEY, config[\"partition_keys\"])\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,CUSTOM_KEY_GENERATOR_CLASS_OPT_VAL) \n",
    "      .option(PARTITIONPATH_FIELD_OPT_KEY,\"route_id:simple, tstamp:timestamp\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e64cb3ce7f4879a7fd1767cd68695c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 5)\n",
    "      .option(HIVE_PARTITION_FIELDS_OPT_KEY, config[\"partition_keys\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,CUSTOM_KEY_GENERATOR_CLASS_OPT_VAL)    \n",
    "      .option(PARTITIONPATH_FIELD_OPT_KEY, \"route_id:simple, tstamp:timestamp\")\n",
    "      .option(\"hoodie.deltastreamer.keygen.timebased.timestamp.type\",\"DATE_STRING\")\n",
    "      .option(\"hoodie.deltastreamer.keygen.timebased.output.dateformat\",\"yyyy-MM-dd\")\n",
    "      .option(\"hoodie.deltastreamer.keygen.timebased.input.dateformat\",\"yyyy-MM-dd kk:mm:ss\")\n",
    "      .option(\"hoodie.datasource.write.hive_style_partitioning\", \"true\")\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7555cfb55f8e46e5a2e4c3715d01ef1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------+-------+\n",
      "|col_name               |data_type|comment|\n",
      "+-----------------------+---------+-------+\n",
      "|_hoodie_commit_time    |string   |null   |\n",
      "|_hoodie_commit_seqno   |string   |null   |\n",
      "|_hoodie_record_key     |string   |null   |\n",
      "|_hoodie_partition_path |string   |null   |\n",
      "|_hoodie_file_name      |string   |null   |\n",
      "|destination            |string   |null   |\n",
      "|route_id               |string   |null   |\n",
      "|trip_id                |bigint   |null   |\n",
      "|routeid                |string   |null   |\n",
      "|tstamp                 |string   |null   |\n",
      "|# Partition Information|         |       |\n",
      "|# col_name             |data_type|comment|\n",
      "|routeid                |string   |null   |\n",
      "|tstamp                 |string   |null   |\n",
      "+-----------------------+---------+-------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe table \"+config['table_name']).show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the partitions fields are present in our Hive table. \n",
    "\n",
    "```\n",
    "Partition Information\n",
    "col_name : route_id\n",
    "col_name : tstamp\n",
    "```\n",
    "\n",
    "Let's now query the data and group by the the partition columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25502e26aef41f18380e19044c4a659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+\n",
      "|route_id|tstamp    |num_trips|\n",
      "+--------+----------+---------+\n",
      "|A       |2022-03-26|200000   |\n",
      "|B       |2022-03-26|200000   |\n",
      "|C       |2022-03-26|200000   |\n",
      "|D       |2022-03-26|200000   |\n",
      "|E       |2022-03-26|200000   |\n",
      "|F       |2022-03-26|200000   |\n",
      "|G       |2022-03-26|200000   |\n",
      "|H       |2022-03-26|200000   |\n",
      "|I       |2022-03-26|200000   |\n",
      "|J       |2022-03-26|200000   |\n",
      "+--------+----------+---------+"
     ]
    }
   ],
   "source": [
    "result_df=spark.sql(\"select route_id,tstamp, count(*) as num_trips from \"+config['table_name']+\" group by route_id, tstamp order by route_id\")\n",
    "result_df.show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the S3 path\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<S3 Bucket>/hudi/hudi_partitioned_trips_table/\n",
    "                           PRE .hoodie/\n",
    "                           PRE route_id=A/\n",
    "                           PRE route_id=B/\n",
    "                           PRE route_id=C/\n",
    "                           PRE route_id=D/\n",
    "                           PRE route_id=E/\n",
    "                           PRE route_id=F/\n",
    "                           PRE route_id=G/\n",
    "                           PRE route_id=H/\n",
    "                           PRE route_id=I/\n",
    "                           PRE route_id=J/\n",
    "2021-12-09 11:32:42          0 .hoodie_$folder$\n",
    "2021-12-09 11:34:12          0 route_id=A_$folder$\n",
    "2021-12-09 11:34:17          0 route_id=B_$folder$\n",
    "2021-12-09 11:34:12          0 route_id=C_$folder$\n",
    "2021-12-09 11:34:15          0 route_id=D_$folder$\n",
    "2021-12-09 11:34:18          0 route_id=E_$folder$\n",
    "2021-12-09 11:34:23          0 route_id=F_$folder$\n",
    "2021-12-09 11:34:23          0 route_id=G_$folder$\n",
    "2021-12-09 11:34:26          0 route_id=H_$folder$\n",
    "2021-12-09 11:34:30          0 route_id=I_$folder$\n",
    "2021-12-09 11:34:33          0 route_id=J_$folder$\n",
    "\n",
    "\n",
    "$ aws s3 ls s3://<S3 Bucket>/hudi/hudi_partitioned_trips_table/route_id=A/\n",
    "                           PRE tstamp=2021-12-09/\n",
    "2021-12-09 11:34:13          0 tstamp=2021-12-09_$folder$\n",
    "\n",
    "\n",
    "$ aws s3 ls s3://<S3 Bucket>/hudi/hudi_partitioned_trips_table/route_id=A/tstamp=2021-12-09/\n",
    "2021-12-09 11:34:13         93 .hoodie_partition_metadata\n",
    "2021-12-09 11:34:17    1729933 39d96271-824b-449c-98ae-e3f931bf8cee-0_0-39-112_20211209193240.parquet\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The other operations Insert, Upsert etc. behave the same way on Partitioned tables.\n",
    "\n",
    "Lets insert new reccords into partitioned table. Before that lets take a look at the record count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e320573ebe324796b0927be73fe9350f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|2000000 |\n",
      "+--------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from \" +config['table_name'] ).show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5598f287cc4100927678a4c1a87ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+-------------------+\n",
      "|destination|route_id|trip_id|             tstamp|\n",
      "+-----------+--------+-------+-------------------+\n",
      "|   Syracuse|       A|2000000|2022-03-26 03:22:03|\n",
      "|   Syracuse|       B|2000001|2022-03-26 03:22:03|\n",
      "|   Syracuse|       C|2000002|2022-03-26 03:22:03|\n",
      "|   Syracuse|       D|2000003|2022-03-26 03:22:03|\n",
      "|   Syracuse|       E|2000004|2022-03-26 03:22:03|\n",
      "|   Syracuse|       F|2000005|2022-03-26 03:22:03|\n",
      "|   Syracuse|       G|2000006|2022-03-26 03:22:03|\n",
      "|   Syracuse|       H|2000007|2022-03-26 03:22:03|\n",
      "|   Syracuse|       I|2000008|2022-03-26 03:22:03|\n",
      "|   Syracuse|       J|2000009|2022-03-26 03:22:03|\n",
      "+-----------+--------+-------+-------------------+"
     ]
    }
   ],
   "source": [
    "insert_dest = [\"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\"]\n",
    "df2 = create_json_df(spark, get_json_data(2000000, 10, insert_dest))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9e2ab416a04650a705b6319d767cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df2.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 10)\n",
    "      .option(HIVE_PARTITION_FIELDS_OPT_KEY, config[\"partition_keys\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,CUSTOM_KEY_GENERATOR_CLASS_OPT_VAL)    \n",
    "      .option(PARTITIONPATH_FIELD_OPT_KEY, \"route_id:simple, tstamp:timestamp\")\n",
    "      .option(\"hoodie.deltastreamer.keygen.timebased.timestamp.type\",\"DATE_STRING\")\n",
    "      .option(\"hoodie.deltastreamer.keygen.timebased.output.dateformat\",\"yyyy-MM-dd\")\n",
    "      .option(\"hoodie.deltastreamer.keygen.timebased.input.dateformat\",\"yyyy-MM-dd kk:mm:ss\")\n",
    "      .option(\"hoodie.datasource.write.hive_style_partitioning\", \"true\")\n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641667bb680844b4b18187aa87b4db1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|2000010 |\n",
      "+--------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from \" +config['table_name'] ).show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4214f6abdcef481b9cf3572b632d23d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+\n",
      "|route_id|tstamp    |num_trips|\n",
      "+--------+----------+---------+\n",
      "|A       |2022-03-26|200001   |\n",
      "|B       |2022-03-26|200001   |\n",
      "|C       |2022-03-26|200001   |\n",
      "|D       |2022-03-26|200001   |\n",
      "|E       |2022-03-26|200001   |\n",
      "|F       |2022-03-26|200001   |\n",
      "|G       |2022-03-26|200001   |\n",
      "|H       |2022-03-26|200001   |\n",
      "|I       |2022-03-26|200001   |\n",
      "|J       |2022-03-26|200001   |\n",
      "+--------+----------+---------+"
     ]
    }
   ],
   "source": [
    "result_df=spark.sql(\"select route_id,tstamp, count(*) as num_trips from \"+config['table_name']+\" group by route_id, tstamp order by route_id\")\n",
    "result_df.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
