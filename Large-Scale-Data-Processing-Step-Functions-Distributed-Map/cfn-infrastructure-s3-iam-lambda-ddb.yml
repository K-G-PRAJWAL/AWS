AWSTemplateFormatVersion: "2010-09-09"
Transform: AWS::Serverless-2016-10-31
Description: AWS Step Functions project for Large-Scale-Data-Processing-Step-Functions-Distributed-Map

Mappings:
  RegionMap: 
      us-east-1:
        manifest: ws-assets-prod-iad-r-iad-ed304a55c2ca1aee
      us-west-2:
        manifest: ws-assets-prod-iad-r-pdx-f3b3f9f1a7d6a3d0
      us-east-2:
        manifest: ws-assets-prod-iad-r-cmh-8d6e9c21a4dec77d
      eu-west-1:
        manifest: ws-assets-prod-iad-r-dub-85e3be25bd827406
      ap-southeast-1:
        manifest: ws-assets-prod-iad-r-sin-694a125e41645312
      ap-southeast-2:
        manifest: ws-assets-prod-iad-r-syd-b04c62a5f16f7b2e
      ca-central-1:
        manifest: ws-assets-prod-iad-r-yul-5c2977cd61bca1f3

Globals:
  Function:
    CodeUri: functions/temps/
    Runtime: python3.9
    Timeout: 120
    Architectures:
      - arm64

Resources:
  # Resources for the state machine that copies data from the public NOAA bucket

  DistributedMapWorkshopDataset:
    Type: AWS::S3::Bucket
    Properties:
      AccessControl: Private
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256

  # Resources for the NOAA data state machine

  DistributedMapResultsBucket:
    Type: AWS::S3::Bucket
    Properties:
      AccessControl: Private
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      VersioningConfiguration:
        Status: Enabled

  ResultsDynamoDBTable:
    Type: AWS::Serverless::SimpleTable
    Properties:
      PrimaryKey:
        Name: pk
        Type: String

  TemperatureStateMachineRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: "Allow"
            Principal:
              AWS: !Ref "AWS::AccountId"
              Service:
                - states.amazonaws.com
            Action:
              - "sts:AssumeRole"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/CloudWatchFullAccess
        - arn:aws:iam::aws:policy/AWSXrayFullAccess
      Policies:
        - PolicyName: ReadDataPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - S3:GetObject
                  - S3:ListBucket
                Resource:
                  - !GetAtt DistributedMapWorkshopDataset.Arn
                  - !Join ["/", [!GetAtt DistributedMapWorkshopDataset.Arn, "*"]]
        - PolicyName: WriteResultsPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - S3:PutObject
                Resource: !Join ["/", [!GetAtt DistributedMapResultsBucket.Arn, "*"]]
        - PolicyName: StartExecutionPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - states:StartExecution
                Resource: !Sub "arn:aws:states:${AWS::Region}:${AWS::AccountId}:stateMachine:DistributedMap-WeatherAnalysis"
        - PolicyName: InvokeMapperReducerPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - lambda:InvokeFunction
                Resource:
                  - !Join [":", [!GetAtt TemperaturesFunction.Arn, "*"]]
                  - !Join [":", [!GetAtt ReducerFunction.Arn, "*"]]

  TemperaturesFunction:
    Type: AWS::Serverless::Function
    Properties:
      InlineCode: |
          import json
          import os
          import csv
          import io

          from datetime import datetime
          from decimal import Decimal
          from typing import Dict, List

          import boto3


          S3_CLIENT = boto3.client("s3")


          def lambda_handler(event: dict, context):
              """Handler that will find the weather station that has the highest average temperature by month.

              Returns a dictionary with "year-month" as the key and dictionary (weather station info) as value.

              """
              input_bucket_name = os.environ["INPUT_BUCKET_NAME"]

              high_by_month: Dict[str, Dict] = {}

              for item in event["Items"]:
                  csv_data = get_file_from_s3(input_bucket_name, item["Key"])
                  dict_data = get_csv_dict_from_string(csv_data)

                  for row in dict_data:
                      avg_temp = float(row["TEMP"])

                      date = datetime.fromisoformat(row["DATE"])
                      month_str = date.strftime("%Y-%m")

                      monthly_high_record = high_by_month.get(month_str) or {}

                      if not monthly_high_record:
                          row["TEMP"] = avg_temp
                          high_by_month[month_str] = row
                          continue

                      if avg_temp > float(monthly_high_record["TEMP"]):
                          high_by_month[month_str] = row

              return high_by_month


          def reducer_handler(event: dict, context: dict):
              """Reducer function will read all of the mapped results from S3 and write to DDB.

              Args:
                  event (dict): The event payload that arrives after the distributed map run has the
                  folllowing structure:

                      {
                      "MapRunArn": "arn-of-the-map-run",
                      "ResultWriterDetails": {
                          "Bucket": "bucket-name-where-results-are-written",
                          "Key": "results/dee8fb57-3653-3f09-88dd-4f39225d2367/manifest.json",
                      },
                  }
                  context (dict): Lambda context
              """
              print(event)
              results_bucket = event["ResultWriterDetails"]["Bucket"]
              manifest = get_file_from_s3(
                  results_bucket,
                  event["ResultWriterDetails"]["Key"],
              )

              maniftest_json = json.loads(manifest)

              high_by_month: Dict[str, Dict] = {}

              for result in maniftest_json["ResultFiles"].get("SUCCEEDED", []):
                  results = get_file_from_s3(results_bucket, result["Key"])

                  for json_result in json.loads(results):

                      monthly_highs: Dict[str, Dict] = json.loads(json_result["Output"])

                      for month_str, row in monthly_highs.items():
                          high_temp = float(row["TEMP"])

                          monthly_high = high_by_month.get(month_str)

                          if not monthly_high:
                              high_by_month[month_str] = row
                              continue

                          if high_temp > float(monthly_high["TEMP"]):
                              high_by_month[month_str] = row

              _write_results_to_ddb(high_by_month)


          def _write_results_to_ddb(high_by_month: Dict[str, Dict]):
              dynamodb = boto3.resource("dynamodb")
              table = dynamodb.Table(os.environ["RESULTS_DYNAMODB_TABLE_NAME"])

              for month_str, row in high_by_month.items():
                  row["pk"] = month_str
                  row["TEMP"] = round(Decimal(row["TEMP"]), 1)
                  table.put_item(Item=row)


          def get_file_from_s3(input_bucket_name: str, key: str) -> str:
              resp = S3_CLIENT.get_object(Bucket=input_bucket_name, Key=key)
              return resp["Body"].read().decode("utf-8")
          def get_csv_dict_from_string(csv_string: str) -> dict:
              return csv.DictReader(io.StringIO(csv_string))
      Handler: index.lambda_handler
      MemorySize: 2048
      Environment:
        Variables:
          INPUT_BUCKET_NAME: !Ref DistributedMapWorkshopDataset
      Policies:
        - S3ReadPolicy:
            BucketName: !Ref DistributedMapWorkshopDataset

  ReducerFunction:
    Type: AWS::Serverless::Function
    Properties:
      InlineCode: |
          import json
          import os
          import csv
          import io

          from datetime import datetime
          from decimal import Decimal
          from typing import Dict, List

          import boto3


          S3_CLIENT = boto3.client("s3")


          def lambda_handler(event: dict, context):
              """Handler that will find the weather station that has the highest average temperature by month.

              Returns a dictionary with "year-month" as the key and dictionary (weather station info) as value.

              """
              input_bucket_name = os.environ["INPUT_BUCKET_NAME"]

              high_by_month: Dict[str, Dict] = {}

              for item in event["Items"]:
                  csv_data = get_file_from_s3(input_bucket_name, item["Key"])
                  dict_data = get_csv_dict_from_string(csv_data)

                  for row in dict_data:
                      avg_temp = float(row["TEMP"])

                      date = datetime.fromisoformat(row["DATE"])
                      month_str = date.strftime("%Y-%m")

                      monthly_high_record = high_by_month.get(month_str) or {}

                      if not monthly_high_record:
                          row["TEMP"] = avg_temp
                          high_by_month[month_str] = row
                          continue

                      if avg_temp > float(monthly_high_record["TEMP"]):
                          high_by_month[month_str] = row

              return high_by_month


          def reducer_handler(event: dict, context: dict):
              """Reducer function will read all of the mapped results from S3 and write to DDB.

              Args:
                  event (dict): The event payload that arrives after the distributed map run has the
                  folllowing structure:

                      {
                      "MapRunArn": "arn-of-the-map-run",
                      "ResultWriterDetails": {
                          "Bucket": "bucket-name-where-results-are-written",
                          "Key": "results/dee8fb57-3653-3f09-88dd-4f39225d2367/manifest.json",
                      },
                  }
                  context (dict): Lambda context
              """
              print(event)
              results_bucket = event["ResultWriterDetails"]["Bucket"]
              manifest = get_file_from_s3(
                  results_bucket,
                  event["ResultWriterDetails"]["Key"],
              )

              maniftest_json = json.loads(manifest)

              high_by_month: Dict[str, Dict] = {}

              for result in maniftest_json["ResultFiles"].get("SUCCEEDED", []):
                  results = get_file_from_s3(results_bucket, result["Key"])

                  for json_result in json.loads(results):

                      monthly_highs: Dict[str, Dict] = json.loads(json_result["Output"])

                      for month_str, row in monthly_highs.items():
                          high_temp = float(row["TEMP"])

                          monthly_high = high_by_month.get(month_str)

                          if not monthly_high:
                              high_by_month[month_str] = row
                              continue

                          if high_temp > float(monthly_high["TEMP"]):
                              high_by_month[month_str] = row

              _write_results_to_ddb(high_by_month)


          def _write_results_to_ddb(high_by_month: Dict[str, Dict]):
              dynamodb = boto3.resource("dynamodb")
              table = dynamodb.Table(os.environ["RESULTS_DYNAMODB_TABLE_NAME"])

              for month_str, row in high_by_month.items():
                  row["pk"] = month_str
                  row["TEMP"] = round(Decimal(row["TEMP"]), 1)
                  table.put_item(Item=row)


          def get_file_from_s3(input_bucket_name: str, key: str) -> str:
              resp = S3_CLIENT.get_object(Bucket=input_bucket_name, Key=key)
              return resp["Body"].read().decode("utf-8")
          def get_csv_dict_from_string(csv_string: str) -> dict:
              return csv.DictReader(io.StringIO(csv_string))
      Handler: index.reducer_handler
      MemorySize: 2048
      Environment:
        Variables:
          RESULTS_BUCKET_NAME: !Ref DistributedMapResultsBucket
          RESULTS_DYNAMODB_TABLE_NAME: !Ref ResultsDynamoDBTable
      Policies:
        - S3ReadPolicy:
            BucketName: !Ref DistributedMapResultsBucket
        - DynamoDBWritePolicy:
            TableName: !Ref ResultsDynamoDBTable

  # Role to access S3 Batch 
  S3BatchRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: "Allow"
            Principal:
              AWS: !Ref "AWS::AccountId"
              Service:
                - batchoperations.s3.amazonaws.com
            Action:
              - "sts:AssumeRole"
      Policies:
        - PolicyName: ReadDataPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
            - Action:
              - s3:PutObject
              - s3:PutObjectAcl
              - s3:PutObjectTagging
              Effect: Allow
              Resource:
                - !Join ["/", [!GetAtt DistributedMapWorkshopDataset.Arn, "*"]]
            - Action:
              - s3:GetObject
              - s3:GetObjectAcl
              - s3:GetObjectTagging
              - s3:ListBucket
              Effect: Allow
              Resource:
              - arn:aws:s3:::noaa-gsod-pds
              - arn:aws:s3:::noaa-gsod-pds/*
            - Effect: Allow
              Action:
              - s3:GetObject
              - s3:GetObjectVersion
              Resource:
                  !Sub
              - 'arn:aws:s3:::${manifest}/9e0368c0-8c49-4bec-a210-8480b51a34ac/resources/noaa-gsod-pds-inventory.csv'
              - manifest: !FindInMap [RegionMap, !Ref "AWS::Region", manifest]
            - Effect: Allow
              Action:
              - s3:PutObject
              Resource:
              - !Join ["/", [!GetAtt DistributedMapWorkshopDataset.Arn, "*"]]

  FunctionS3Create:
    Type: 'AWS::Serverless::Function'
    Properties:
      Tracing: Active
      InlineCode: |
          // Lambda function used as CloudFormation custom resource to create an S3 object.
          //const AWS = require("aws-sdk");
          //const S3 = new AWS.S3();
          const { Tracer } = require("@aws-lambda-powertools/tracer");
          const tracer = new Tracer({ serviceName: 'module13setup' });
          // Get service clients module and commands using ES6 syntax.
          const { S3Client, ListObjectsCommand, DeleteObjectCommand } = require("@aws-sdk/client-s3");
          const { S3ControlClient, CreateJobCommand } = require("@aws-sdk/client-s3-control");
          const client = new S3ControlClient();
          const response = require("./cfn-response.js");
          const targetClient = tracer.captureAWSv3Client(
              new S3Client({ region: process.env.AWS_REGION })
          )

          function send(event, context, responseStatus, responseData, physicalResourceId, noEcho) {
            try {
              const https = require("https");
              const { URL } = require("url");

              const responseBody = {
                Status: responseStatus,
                Reason: "See the details in CloudWatch Log Stream: " + context.logStreamName,
                PhysicalResourceId: context.logStreamName,
                StackId: event.StackId,
                RequestId: event.RequestId,
                LogicalResourceId: event.LogicalResourceId,
                NoEcho: false,
                Data: responseData,
              };
              console.log("Response body:\n", JSON.stringify(responseBody));

              const parsedUrl = new URL(event.ResponseURL);
              const requestOptions = {
                hostname: parsedUrl.hostname,
                port: 443,
                path: parsedUrl.pathname + parsedUrl.search,
                method: "PUT",
                headers: {
                  "content-type": "",
                  "content-length": JSON.stringify(responseBody).length,
                },
              };
              console.log("Request options:\n", JSON.stringify(requestOptions));

              // Send response back to CloudFormation
              return new Promise((resolve, reject) => {
                const request = https.request(requestOptions, function (response) {
                  response.on("data", () => {});
                  response.on("end", () => {
                    console.log("Status code: ", response.statusCode);
                    console.log("Status message: ", response.statusMessage);
                    resolve("Success");
                  });
                });
                request.on("error", (e) => {
                  console.error(e);
                  reject("Error");
                });
                request.write(JSON.stringify(responseBody));
                request.end();
              });
            } catch (error) {
              console.error("Error in cfn_response:\n", error);
              return;
            }
          };

          exports.handler = async (event, context) => {
              console.log("Event:\n", JSON.stringify(event));
              let responseData = {};
              let responseStatus = response.FAILED;
              // CloudFormation custom resource request types: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/crpg-ref-requesttypes.html
              if (event.RequestType == "Delete") {
                  // If the CloudFormation stack is deleted:
                  try {
                      // To allow the CloudFormation process to continue regardless of the delete operation response,
                      // set the responseStatus to SUCCESS before the delete operation begins.
                      responseStatus = response.SUCCESS;
                      // Required resource properties to delete an S3 object:
                      const bucketParams = {
                          Bucket: process.env.BUCKET_NAME
                      }
                      const s3Contents = await targetClient.send(new ListObjectsCommand(bucketParams));
                      for (let i = 0; i < s3Contents["Contents"].length; i += 1) {
                          const params = {
                              Bucket: process.env.BUCKET_NAME,
                              CopySource: `${process.env.BUCKET_NAME}/${s3Contents["Contents"][i]["Key"]}`,
                              Key: s3Contents["Contents"][i]["Key"]
                          };
                          const data = await targetClient.send(new DeleteObjectCommand(params));
                          //console.log(data)
                      }
                  } catch (error) {
                      console.error("Error during S3 delete:\n", error);
                  }
              } else {
                  // If the CloudFormation stack is created or updated:
                  try {
                      // Create new S3 Batch copy job
                      const jobParams = {
                          AccountId: process.env.ACCOUNT_ID,
                          Operation: {
                              S3PutObjectCopy: {
                                  TargetResource: `arn:aws:s3:::${process.env.BUCKET_NAME}`,
                                  CannedAccessControlList: "private",
                              }
                          },
                          Manifest: {
                              Spec: {
                                  Format: "S3BatchOperations_CSV_20180820",
                                  Fields: ["Bucket", "Key"]
                              },
                              Location: {
                                  ObjectArn: `arn:aws:s3:::${process.env.MANIFEST_BUCKET}/9e0368c0-8c49-4bec-a210-8480b51a34ac/resources/noaa-gsod-pds-inventory.csv`,
                                  ETag: "9922a0f6838b310544951c7a535df7fd",
                              }
                          },
                          ClientRequestToken: process.env.AWS_LAMBDA_FUNCTION_NAME,
                          ConfirmationRequired: false,
                          Description: "Copying S3 objects from bucket 'noaa-gsod-pds' to bucket 'module13setup'",
                          Priority: 1,
                          Report: {
                              Enabled: false,
                          },
                          RoleArn: `arn:aws:iam::${process.env.ACCOUNT_ID}:role/${process.env.ROLE_ARN}`,
                      }

                      console.log(jobParams);
                      const command = new CreateJobCommand(jobParams);
                      const commandResult = await client.send(command);
                      console.log(JSON.stringify(commandResult))

                      // Response data that is sent back to CloudFormation:
                      responseData = { "Response": "Response" }
                      responseStatus = response.SUCCESS;
                  } catch (error) {
                      console.error("Error during S3 upload:\n", error);
                  }
              }
              await send(event, context, responseStatus, responseData);
              return;
          };
      Runtime: nodejs18.x
      MemorySize: 1024
      Timeout: 300
      Handler: index.handler
      Environment:
        Variables:
          BUCKET_NAME: !Ref DistributedMapWorkshopDataset
          MANIFEST_BUCKET: !FindInMap [RegionMap, !Ref "AWS::Region", manifest]
          ROLE_ARN: !Ref S3BatchRole
          ACCOUNT_ID: !Ref AWS::AccountId
      Policies:
        - S3CrudPolicy:
            BucketName: !Ref DistributedMapWorkshopDataset
        - S3ReadPolicy:
            BucketName: noaa-gsod-pds
        - CloudWatchLambdaInsightsExecutionRolePolicy
        - Statement:
          - Sid: S3Batch
            Effect: Allow
            Action:
            - s3:CreateJob
            Resource: "*" 
          - Sid: PassRole
            Effect: "Allow"
            Action: "iam:PassRole"
            Resource: !GetAtt S3BatchRole.Arn
      Layers:
        - !Sub "arn:aws:lambda:${AWS::Region}:580247275435:layer:LambdaInsightsExtension-Arm64:1"
        - !Sub "arn:aws:lambda:${AWS::Region}:094274105915:layer:AWSLambdaPowertoolsTypeScript:11"
  CustomResourceS3Create:
    Type: 'Custom::S3Create'
    Properties:
      ServiceToken: !GetAtt FunctionS3Create.Arn
Outputs:
  DynamoDBTableName:
    Description: DynamoDB table name where final results are written
    Value: !Ref ResultsDynamoDBTable
  DistributedMapWorkshopDataset:
    Description: Bucket where the NOAA data will be copied, and where the analysis will read
    Value: !Ref DistributedMapWorkshopDataset
  StateMachineResultsBucket:
    Description: Bucket where the distributed map run will write results
    Value: !Ref DistributedMapResultsBucket
